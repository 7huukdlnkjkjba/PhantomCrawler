#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PhantomCrawler - éšåŒ¿çˆ¬è™«æ¡†æ¶
ä¸€é”®è°ƒç”¨å…¥å£ | ä¸ƒå®—æ¬²æ ¸å¿ƒå¼•æ“ | å®æˆ˜ä¼˜åŒ–ç‰ˆ
"""

import os
import sys
import argparse
import json
import time
from typing import List, Dict, Any, Optional, Callable

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from src.core.crawler import PhantomCrawler
from src.configs.config import global_config


def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                               â•‘
    â•‘                     PhantomCrawler                            â•‘
    â•‘             é«˜çº§éšåŒ¿çˆ¬è™«æ¡†æ¶ | ä¸ƒå®—æ¬²æ ¸å¿ƒå¼•æ“                 â•‘
    â•‘                                                               â•‘
    â•‘    åŠ¨æ€æŒ‡çº¹ä¼ªè£… | è¡Œä¸ºæ¨¡æ‹Ÿ | åè®®æ··æ·† | å…ƒè®¤çŸ¥ç³»ç»Ÿ            â•‘
    â•‘                                                               â•‘
    â•‘          è­¦å‘Š: è¯·åœ¨åˆæ³•æˆæƒèŒƒå›´å†…ä½¿ç”¨æ­¤å·¥å…·                   â•‘
    â•‘                                                               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)


def load_config_from_file(config_file: str) -> Dict[str, Any]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"[!] é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
        return {}


def save_config_to_file(config: Dict[str, Any], config_file: str) -> bool:
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"[!] é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}")
        return False


def setup_crawler_config(args: argparse.Namespace) -> None:
    """è®¾ç½®çˆ¬è™«é…ç½®"""
    # åŸºç¡€é…ç½®
    if args.timeout:
        global_config.set('request_timeout', args.timeout)
    
    if args.retries:
        global_config.set('max_retries', args.retries)
    
    # æŒ‡çº¹ä¼ªè£…é…ç½®
    global_config.set('fingerprint.enable_dynamic_ua', args.dynamic_ua)
    global_config.set('fingerprint.enable_ja3_simulation', args.ja3)
    global_config.set('fingerprint.enable_browser_fingerprint_spoofing', args.browser_fp)
    
    # è¡Œä¸ºæ¨¡æ‹Ÿé…ç½®
    global_config.set('behavior_simulation.enable_human_delay', args.human_delay)
    global_config.set('behavior_simulation.use_gamma_distribution', args.gamma_delay)
    
    if args.min_delay:
        global_config.set('behavior_simulation.min_delay', args.min_delay)
    
    if args.max_delay:
        global_config.set('behavior_simulation.max_delay', args.max_delay)
    
    # è¯·æ±‚é“¾æ±¡æŸ“
    global_config.set('behavior_simulation.enable_request_chain_pollution', args.request_chain)
    
    # å…ƒè®¤çŸ¥ç³»ç»Ÿ
    global_config.set('metacognition.enabled', args.metacognition)


def process_single_url(crawler: PhantomCrawler, url: str, output_file: Optional[str] = None) -> None:
    """å¤„ç†å•ä¸ªURLçˆ¬å–"""
    print(f"\n[*] å¼€å§‹çˆ¬å–: {url}")
    start_time = time.time()
    
    try:
        # å®šä¹‰å›è°ƒå‡½æ•°
        def response_callback(response: Dict[str, Any]) -> None:
            elapsed = time.time() - start_time
            print(f"[âœ“] çˆ¬å–å®Œæˆ: {url}")
            print(f"[*] çŠ¶æ€ç : {response.get('status_code')}")
            print(f"[*] å“åº”å¤§å°: {len(response.get('content', ''))} å­—èŠ‚")
            print(f"[*] è€—æ—¶: {elapsed:.2f} ç§’")
            
            if output_file:
                # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(response, f, indent=2, ensure_ascii=False)
                print(f"[*] ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        
        # æ‰§è¡Œçˆ¬å–
        crawler.crawl(url, response_callback=response_callback)
        
    except Exception as e:
        print(f"[âœ—] çˆ¬å–å¤±è´¥: {str(e)}")


def process_url_list(crawler: PhantomCrawler, url_list: List[str], output_dir: Optional[str] = None) -> None:
    """å¤„ç†URLåˆ—è¡¨çˆ¬å–"""
    total = len(url_list)
    success = 0
    
    print(f"\n[*] å¼€å§‹æ‰¹é‡çˆ¬å–: {total} ä¸ªURL")
    
    for i, url in enumerate(url_list, 1):
        print(f"\n[{i}/{total}] å¼€å§‹çˆ¬å–: {url}")
        start_time = time.time()
        
        try:
            # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
            output_file = None
            if output_dir:
                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                safe_filename = f"{i:04d}_{url.replace('://', '_').replace('/', '_').replace('?', '_').replace('&', '_')[:50]}.json"
                output_file = os.path.join(output_dir, safe_filename)
            
            # å®šä¹‰å›è°ƒå‡½æ•°
            def response_callback(response: Dict[str, Any], idx=i) -> None:
                nonlocal success
                success += 1
                elapsed = time.time() - start_time
                print(f"[âœ“] çˆ¬å–å®Œæˆ [{idx}/{total}]: {url}")
                print(f"[*] çŠ¶æ€ç : {response.get('status_code')}")
                print(f"[*] è€—æ—¶: {elapsed:.2f} ç§’")
                
                if output_file:
                    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(response, f, indent=2, ensure_ascii=False)
            
            # æ‰§è¡Œçˆ¬å–
            crawler.crawl(url, response_callback=response_callback)
            
        except Exception as e:
            print(f"[âœ—] çˆ¬å–å¤±è´¥ [{i}/{total}]: {str(e)}")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        if i < total:
            delay = global_config.get('behavior_simulation.min_delay', 1.0)
            print(f"[*] ç­‰å¾… {delay} ç§’åç»§ç»­...")
            time.sleep(delay)
    
    print(f"\n[*] æ‰¹é‡çˆ¬å–å®Œæˆ")
    print(f"[*] æˆåŠŸ: {success}, å¤±è´¥: {total - success}")


def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='PhantomCrawler - é«˜çº§éšåŒ¿çˆ¬è™«æ¡†æ¶', 
                                     formatter_class=argparse.RawTextHelpFormatter)
    
    # URLå‚æ•°ç»„
    url_group = parser.add_mutually_exclusive_group(required=True)
    url_group.add_argument('-u', '--url', type=str, help='å•ä¸ªçˆ¬å–ç›®æ ‡URL')
    url_group.add_argument('-l', '--list', type=str, help='åŒ…å«URLåˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    
    # é…ç½®å‚æ•°
    parser.add_argument('-c', '--config', type=str, help='è‡ªå®šä¹‰JSONé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-s', '--save-config', type=str, help='ä¿å­˜å½“å‰é…ç½®åˆ°æŒ‡å®šæ–‡ä»¶')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('-o', '--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å•ä¸ªURLæ—¶ä½¿ç”¨)')
    parser.add_argument('-d', '--output-dir', type=str, help='è¾“å‡ºç›®å½•è·¯å¾„ (æ‰¹é‡URLæ—¶ä½¿ç”¨)')
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument('--no-dynamic-ua', dest='dynamic_ua', action='store_false', 
                       help='ç¦ç”¨åŠ¨æ€User-Agent')
    parser.add_argument('--no-ja3', dest='ja3', action='store_false', 
                       help='ç¦ç”¨JA3æŒ‡çº¹æ¨¡æ‹Ÿ')
    parser.add_argument('--no-browser-fp', dest='browser_fp', action='store_false', 
                       help='ç¦ç”¨æµè§ˆå™¨æŒ‡çº¹æ¬ºéª—')
    parser.add_argument('--no-human-delay', dest='human_delay', action='store_false', 
                       help='ç¦ç”¨äººç±»å»¶è¿Ÿæ¨¡æ‹Ÿ')
    parser.add_argument('--no-gamma', dest='gamma_delay', action='store_false', 
                       help='ç¦ç”¨ä¼½é©¬åˆ†å¸ƒå»¶è¿Ÿ')
    parser.add_argument('--no-request-chain', dest='request_chain', action='store_false', 
                       help='ç¦ç”¨è¯·æ±‚é“¾æ±¡æŸ“')
    parser.add_argument('--no-metacognition', dest='metacognition', action='store_false', 
                       help='ç¦ç”¨å…ƒè®¤çŸ¥ç³»ç»Ÿ')
    
    # é«˜çº§æµ‹è¯•æ¨¡å—
    parser.add_argument('--advanced', action='store_true', 
                       help='æ¿€æ´»é«˜çº§æµ‹è¯•æ¨¡å—ï¼ˆè­¦å‘Šï¼šéœ€æˆæƒä½¿ç”¨ï¼‰')
    parser.add_argument('--recursive-test', action='store_true', 
                       help='å¯ç”¨é€’å½’è·¯å¾„æµ‹è¯•æ¨¡å¼')
    
    # æ•°å€¼å‚æ•°
    parser.add_argument('--timeout', type=int, help='è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)')
    parser.add_argument('--retries', type=int, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--min-delay', type=float, help='æœ€å°å»¶è¿Ÿæ—¶é—´ (ç§’)')
    parser.add_argument('--max-delay', type=float, help='æœ€å¤§å»¶è¿Ÿæ—¶é—´ (ç§’)')
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--stealth', action='store_true', help='å¯ç”¨æœ€é«˜çº§åˆ«çš„éšåŒ¿æ¨¡å¼')
    parser.add_argument('--aggressive', action='store_true', help='å¯ç”¨æ¿€è¿›çˆ¬å–æ¨¡å¼')
    parser.add_argument('--balanced', action='store_true', help='å¯ç”¨å¹³è¡¡æ¨¡å¼ (é»˜è®¤)')
    
    # è®¾ç½®é»˜è®¤å€¼
    parser.set_defaults(
        dynamic_ua=True,
        ja3=True,
        browser_fp=True,
        human_delay=True,
        gamma_delay=True,
        request_chain=True,
        metacognition=True
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½å®ƒ
    if args.config:
        custom_config = load_config_from_file(args.config)
        # åº”ç”¨è‡ªå®šä¹‰é…ç½®åˆ°å…¨å±€é…ç½®
        for key, value in custom_config.items():
            global_config.set(key, value)
    
    # æ ¹æ®æ¨¡å¼é¢„è®¾é…ç½®
    if args.stealth:
        print("\n[*] å¯ç”¨éšåŒ¿æ¨¡å¼ - æœ€é«˜çº§åˆ«åæ£€æµ‹é…ç½®")
        args.dynamic_ua = True
        args.ja3 = True
        args.browser_fp = True
        args.human_delay = True
        args.gamma_delay = True
        args.request_chain = True
        args.metacognition = True
        if not args.min_delay:
            args.min_delay = 2.0
        if not args.max_delay:
            args.max_delay = 8.0
    
    elif args.aggressive:
        print("\n[*] å¯ç”¨æ¿€è¿›æ¨¡å¼ - è¿½æ±‚é€Ÿåº¦å’Œæ•ˆç‡")
        args.human_delay = False
        args.request_chain = False
        if not args.timeout:
            args.timeout = 15
        if not args.retries:
            args.retries = 2
    
    elif args.balanced or not any([args.stealth, args.aggressive]):
        print("\n[*] å¯ç”¨å¹³è¡¡æ¨¡å¼ - æ€§èƒ½ä¸éšåŒ¿æ€§çš„å¹³è¡¡")
        # é»˜è®¤å°±æ˜¯å¹³è¡¡æ¨¡å¼ï¼Œä¸éœ€è¦ç‰¹åˆ«è®¾ç½®
    
    # è®¾ç½®çˆ¬è™«é…ç½®
    setup_crawler_config(args)
    
    # å¦‚æœéœ€è¦ä¿å­˜é…ç½®
    if args.save_config:
        # è·å–å½“å‰é…ç½®
        current_config = {
            'fingerprint.enable_dynamic_ua': args.dynamic_ua,
            'fingerprint.enable_ja3_simulation': args.ja3,
            'fingerprint.enable_browser_fingerprint_spoofing': args.browser_fp,
            'behavior_simulation.enable_human_delay': args.human_delay,
            'behavior_simulation.use_gamma_distribution': args.gamma_delay,
            'behavior_simulation.enable_request_chain_pollution': args.request_chain,
            'metacognition.enabled': args.metacognition
        }
        
        if args.timeout:
            current_config['request_timeout'] = args.timeout
        if args.retries:
            current_config['max_retries'] = args.retries
        if args.min_delay:
            current_config['behavior_simulation.min_delay'] = args.min_delay
        if args.max_delay:
            current_config['behavior_simulation.max_delay'] = args.max_delay
        
        save_config_to_file(current_config, args.save_config)
        print(f"\n[*] é…ç½®å·²ä¿å­˜è‡³: {args.save_config}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if args.output_dir and not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆ›å»ºå¹¶åˆå§‹åŒ–çˆ¬è™«
    print("\n[*] åˆå§‹åŒ–PhantomCrawler...")
    crawler = PhantomCrawler()
    
    if not crawler.initialize():
        print("\n[âœ—] çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼")
        sys.exit(1)
    
    print("[âœ“] çˆ¬è™«åˆå§‹åŒ–æˆåŠŸï¼")
    
    # æ¿€æ´»é«˜çº§æµ‹è¯•æ¨¡å—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.advanced:
        print("\nâš ï¸  è­¦å‘Šï¼šæ­£åœ¨æ¿€æ´»é«˜çº§æµ‹è¯•æ¨¡å—ï¼")
        print("âš ï¸  æ­¤æ¨¡å¼ç”¨äºæˆæƒå®‰å…¨æµ‹è¯•ï¼Œå¿…é¡»åœ¨è·å¾—æ˜ç¡®æˆæƒçš„ç³»ç»Ÿä¸Šè¿è¡Œï¼")
        
        # äºŒæ¬¡ç¡®è®¤
        confirm = input("\nè¯·è¾“å…¥ 'YES' ç¡®è®¤æ¿€æ´»é«˜çº§æµ‹è¯•æ¨¡å—: ")
        if confirm.upper() == 'YES':
            if hasattr(crawler.seven_desires, 'activate_advanced_testing'):
                crawler.seven_desires.activate_advanced_testing()
                print("\n[!] é«˜çº§æµ‹è¯•æ¨¡å—å·²å®Œå…¨æ¿€æ´»ï¼")
            elif hasattr(crawler.seven_desires, 'awaken_hatred'):  # å…¼å®¹æ—§æ–¹æ³•å
                print("\n[!] æ­£åœ¨ä½¿ç”¨å…¼å®¹æ¨¡å¼æ¿€æ´»é«˜çº§æµ‹è¯•åŠŸèƒ½ï¼")
                crawler.seven_desires.awaken_hatred()
            else:
                print("\n[âœ—] é«˜çº§æµ‹è¯•æ¨¡å—æœªæ‰¾åˆ°ï¼")
        else:
            print("\n[*] é«˜çº§æµ‹è¯•æ¨¡å—æ¿€æ´»å·²å–æ¶ˆ")
            sys.exit(0)
    
    try:
        # å¤„ç†çˆ¬å–ä»»åŠ¡
        if args.url:
            # å•ä¸ªURLçˆ¬å–
            process_single_url(crawler, args.url, args.output)
        elif args.list:
            # æ‰¹é‡URLçˆ¬å–
            try:
                with open(args.list, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip()]
                
                if not urls:
                    print("\n[!] URLåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼")
                    sys.exit(1)
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€’å½’è·¯å¾„æµ‹è¯•æ¨¡å¼
                if args.recursive_test and hasattr(crawler, 'crawl_iterative'):
                    print("\nğŸ”— å¯ç”¨é€’å½’è·¯å¾„æµ‹è¯•æ¨¡å¼ï¼")
                    
                    # ä»ç¬¬ä¸€ä¸ªURLå¼€å§‹é€’å½’æµ‹è¯•
                    start_url = urls[0]
                    print(f"\n[*] å¼€å§‹é€’å½’è·¯å¾„æµ‹è¯•ï¼Œèµ·å§‹URL: {start_url}")
                    
                    # æ‰§è¡Œé€’å½’è·¯å¾„æµ‹è¯•çˆ¬å–
                    results = crawler.crawl_iterative(start_url, max_depth=2, max_urls=50)
                    
                    # æ˜¾ç¤ºç»“æœ
                    print("\n[*] é€’å½’è·¯å¾„æµ‹è¯•å®Œæˆï¼")
                    print(f"[*] æ€»è®¡å¤„ç†URL: {results.get('total_processed', 0)}")
                    print(f"[*] å¤±è´¥URL: {results.get('total_errors', 0)}")
                    print(f"[*] è¾¾åˆ°æ·±åº¦: {results.get('depth_reached', 0)}")
                else:
                    # æ™®é€šæ‰¹é‡çˆ¬å–
                    process_url_list(crawler, urls, args.output_dir)
                
            except Exception as e:
                print(f"\n[âœ—] URLåˆ—è¡¨æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
                sys.exit(1)
    
    except KeyboardInterrupt:
        print("\n[*] ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\n[âœ—] å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
        
        # å¦‚æœå¯ç”¨äº†æ™ºèƒ½ç­–ç•¥ä¼˜åŒ–ï¼Œè®°å½•å¤±è´¥ä»¥ä¾¿å­¦ä¹ 
        if hasattr(crawler, 'seven_desires') and hasattr(crawler.seven_desires, 'optimize_testing_strategy'):
            crawler.seven_desires.optimize_testing_strategy(str(e))
    finally:
        print("\n[*] PhantomCrawler å·²å…³é—­")


if __name__ == "__main__":
    main()