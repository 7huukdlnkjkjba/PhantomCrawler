# PhantomCrawler - ä¸ƒå®—æ¬²æ ¸å¿ƒå¼•æ“ | å®æˆ˜çªç ´ä¼˜åŒ–ç‰ˆ
import asyncio
import random
import time
import importlib.util
from typing import Dict, List, Optional, Any, Callable, Set
import httpx
from urllib.parse import urlparse
from src.modules.evasion.fingerprint_spoofer import FingerprintSpoofer
from src.modules.behavior.behavior_simulator import BehaviorSimulator
from src.modules.evasion.protocol_obfuscator import ProtocolObfuscator
from src.modules.parsing.html_parser import HTMLParser
from src.config import global_config

# åŠ¨æ€æ£€æŸ¥playwrightæ˜¯å¦å®‰è£…
HAS_PLAYWRIGHT = importlib.util.find_spec('playwright') is not None

class PhantomCrawler:
    """PhantomCrawleræ ¸å¿ƒå¼•æ“ç±»ï¼Œæ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—"""
    
    def __init__(self, config_file: Optional[str] = None, auto_initialize: bool = True):
        # åˆå§‹åŒ–é…ç½®
        try:
            if config_file:
                from src.configs.config import Config
                Config(config_file)  # è¿™å°†æ›´æ–°å…¨å±€é…ç½®
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {str(e)}")
        
        # åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—ï¼ˆå¸¦æœ‰å¼‚å¸¸å¤„ç†ï¼‰
        self.fingerprint_spoofer = None
        self.behavior_simulator = None
        self.protocol_obfuscator = None
        self.seven_desires = None
        
        try:
            self.fingerprint_spoofer = FingerprintSpoofer()
            self.behavior_simulator = BehaviorSimulator()
            self.protocol_obfuscator = ProtocolObfuscator()
            
            # å®‰å…¨è·å–ä¸ƒå®—æ¬²å¼•æ“å®ä¾‹
            if self.behavior_simulator and hasattr(self.behavior_simulator, 'seven_desires'):
                self.seven_desires = self.behavior_simulator.seven_desires
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ ¸å¿ƒæ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†å¤±è´¥: {str(e)}")
        
        # çˆ¬è™«çŠ¶æ€ï¼ˆå®æˆ˜ä¼˜åŒ–ï¼‰
        self.is_running = False
        self.session_id = self._generate_session_id()
        self.crawl_history = []
        self.success_streak = 0
        self.total_attempts = 0
        self.consecutive_failures = 0
        
        # æ–°å¢å®æˆ˜çŠ¶æ€æŒ‡æ ‡
        self.current_retry_round = 0
        self.max_retry_rounds = 3
        self.retry_interval_base = 2.0
        self.playwright_available = HAS_PLAYWRIGHT
        self.fingerprint_rotation_interval = 300  # é»˜è®¤5åˆ†é’Ÿè½®æ¢ä¸€æ¬¡æŒ‡çº¹
        
        # å®æˆ˜ä¼˜åŒ–çš„ç­–ç•¥é…ç½®
        self.current_strategies = {
            'fingerprint': {'advanced': False, 'rotation_interval': 0, 'last_rotated': 0},
            'delay': 2.0,
            'request_chain': [],
            'playwright_strategy': 'normal',  # normal, stealth, aggressive
            'evasion_level': 0,  # 0-3ï¼Œåæ£€æµ‹ç­‰çº§
            'risk_adjusted': False,
            'proxy_usage': 'auto',  # auto, required, disabled
            'timeout_seconds': 30
        }
        
        # é€’å½’é˜²æŠ¤æ ‡å¿—ï¼ˆå®æˆ˜å¢å¼ºï¼‰
        self._analysis_in_progress = False
        self._record_failure_in_progress = False
        self._last_reset_time = time.time()
        self._last_fingerprint_rotation = time.time()
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        dominant_desire = getattr(self.seven_desires, 'dominant_desire', 'æœªçŸ¥') if self.seven_desires else 'æœªçŸ¥'
        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] åˆå§‹åŒ–å®Œæˆï¼Œå½“å‰ä¸»å¯¼æ¬²æœ›: {dominant_desire}")
        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] Playwrightæ”¯æŒ: {'å·²å¯ç”¨' if self.playwright_available else 'æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ'}")
        
        # å®¢æˆ·ç«¯å®ä¾‹
        self.http_client = None
        self.playwright_browser = None
        
        # HTMLè§£æå™¨
        self.html_parser = HTMLParser()
        
        # å­¦ä¹ çŠ¶æ€
        self.previous_state = None
        self.previous_action = None
        
        # è‡ªåŠ¨åˆå§‹åŒ–
        if auto_initialize:
            self.initialize()
    
    def _generate_session_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID"""
        import uuid
        return str(uuid.uuid4())[:8]
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–çˆ¬è™«"""
        try:
            # åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯
            self.http_client = self._create_http_client()
            
            self.is_running = True
            print(f"[PhantomCrawler] åˆå§‹åŒ–æˆåŠŸï¼Œä¼šè¯ID: {self.session_id}")
            return True
        except Exception as e:
            print(f"[PhantomCrawler] åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _create_http_client(self) -> httpx.Client:
        """åˆ›å»ºé…ç½®å¥½çš„HTTPå®¢æˆ·ç«¯"""
        try:
            # ç”Ÿæˆæµè§ˆå™¨æŒ‡çº¹
            headers = self.fingerprint_spoofer.generate_fingerprint()
            
            # åˆ›å»ºä»£ç†å®¢æˆ·ç«¯
            client = self.protocol_obfuscator.create_proxied_httpx_client()
            
            # åº”ç”¨æŒ‡çº¹
            client = self.fingerprint_spoofer.configure_httpx_client(client)
            client.headers.update(headers)
            
            return client
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] åˆ›å»ºHTTPå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            # åˆ›å»ºæœ€å°åŠŸèƒ½çš„å®¢æˆ·ç«¯ä½œä¸ºå¤‡ä»½
            return httpx.Client(timeout=30, follow_redirects=True)
    
    def _reset_session(self) -> None:
        """é‡ç½®çˆ¬è™«ä¼šè¯ - å®æˆ˜ä¼˜åŒ–ï¼šé¿å…é•¿æ—¶é—´è¿è¡Œçš„èµ„æºæ³„éœ²"""
        try:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ‰§è¡Œä¼šè¯é‡ç½®ï¼Œæ¸…ç†èµ„æº...")
            
            # å…³é—­å¹¶é‡æ–°åˆ›å»ºHTTPå®¢æˆ·ç«¯
            if hasattr(self, 'http_client') and self.http_client:
                try:
                    self.http_client.close()
                except:
                    pass
            self.http_client = self._create_http_client()
            
            # åˆ·æ–°èº«ä»½ä¿¡æ¯
            self._refresh_identity()
            
            # é‡ç½®çŠ¶æ€è®°å½•
            self.current_retry_round = 0
            self.success_streak = 0
            
            # æ›´æ–°é‡ç½®æ—¶é—´æˆ³
            self._last_reset_time = time.time()
            
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] ä¼šè¯é‡ç½®å®Œæˆ")
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] ä¼šè¯é‡ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    def _record_crawl_history(self, url: str, response: httpx.Response, response_time: float, blocked: bool) -> None:
        """å®‰å…¨è®°å½•çˆ¬å–å†å²"""
        try:
            history_entry = {
                'url': url,
                'status_code': response.status_code,
                'timestamp': time.time(),
                'session_id': self.session_id,
                'blocked': blocked,
                'response_time': response_time
            }
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼Œé¿å…å†…å­˜æº¢å‡º
            if hasattr(self, 'crawl_history'):
                self.crawl_history.append(history_entry)
                # åªä¿ç•™æœ€è¿‘1000æ¡è®°å½•
                if len(self.crawl_history) > 1000:
                    self.crawl_history = self.crawl_history[-1000:]
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è®°å½•å†å²å¤±è´¥: {str(e)}")
    
    def _apply_safe_default_strategy(self) -> None:
        """åº”ç”¨å®‰å…¨çš„é»˜è®¤ç­–ç•¥ï¼Œå½“ç­–ç•¥ç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨"""
        try:
            self.current_strategies = {
                'timeout_seconds': 30,
                'retry_count': 2,
                'delay_seconds': 1.5,
                'proxy_strategy': 'rotate_per_attempt',
                'fingerprint_strategy': 'regular',
                'playwright_strategy': 'stealth'
            }
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] å·²åº”ç”¨å®‰å…¨é»˜è®¤ç­–ç•¥")
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] åº”ç”¨é»˜è®¤ç­–ç•¥å¤±è´¥: {str(e)}")
    
    def _smart_fingerprint_rotation(self) -> None:
        """æ™ºèƒ½è½®æ¢æŒ‡çº¹ï¼Œé¿å…è¢«æ£€æµ‹"""
        pass
    
    def crawl_iterative(self, start_url: str, max_depth: int = 3, max_urls: int = 100) -> Dict[str, Any]:
        """
        é€’å½’è·¯å¾„æµ‹è¯•æ¨¡å¼ - ä»ä¸€ä¸ªç½‘ç«™æå–é“¾æ¥ï¼Œè‡ªåŠ¨æ¢ç´¢ä¸‹ä¸€ä¸ªç›®æ ‡
        
        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
            max_urls: æœ€å¤§å¤„ç†çš„URLæ•°é‡
        
        Returns:
            çˆ¬å–ç»“æœå­—å…¸
        """
        import threading
        import queue
        from urllib.parse import urljoin, urlparse
        
        results = {'results': {}, 'errors': [], 'depth_reached': 0}
        visited_urls = set()
        url_queue = queue.Queue()
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†é«˜çº§æµ‹è¯•ç­–ç•¥
        advanced_testing_enabled = False
        if self.seven_desires and hasattr(self.seven_desires, 'testing_strategies'):
            advanced_testing_enabled = self.seven_desires.testing_strategies.get('recursive_path_testing', False)
            
            # å¦‚æœå¯ç”¨äº†èµ„æºå‹åŠ›æµ‹è¯•ï¼Œè°ƒæ•´å‚æ•°
            if self.seven_desires.testing_strategies.get('resource_stress_testing', False):
                # è°ƒæ•´å»¶è¿Ÿï¼Œå¢åŠ è¯·æ±‚é¢‘ç‡è¿›è¡Œå‹åŠ›æµ‹è¯•
                self.current_strategies['delay'] = 0.01
                max_urls = 1000  # å¤§é‡URLç”¨äºå…¨é¢æµ‹è¯•
        
        if advanced_testing_enabled:
            print("ğŸ”— é€’å½’è·¯å¾„æµ‹è¯•å·²æ¿€æ´»ï¼šå¼€å§‹ä»ä¸€ä¸ªç½‘ç«™æ¢ç´¢åˆ°å¦ä¸€ä¸ªç½‘ç«™")
        
        # æ·»åŠ èµ·å§‹URL
        url_queue.put((start_url, 0))
        visited_urls.add(start_url)
        
        processed_count = 0
        
        while not url_queue.empty() and processed_count < max_urls:
            url, depth = url_queue.get()
            results['depth_reached'] = max(results['depth_reached'], depth)
            
            if depth > max_depth:
                continue
            
            try:
                print(f"[é€’å½’è·¯å¾„æµ‹è¯•] æ­£åœ¨æµ‹è¯•: {url} (æ·±åº¦: {depth})")
                
                # æ‰§è¡Œçˆ¬å–
                result = self.crawl(url)
                
                # ä¿å­˜ç»“æœ
                results['results'][url] = {
                    'status_code': result.get('status_code'),
                    'depth': depth,
                    'timestamp': time.time()
                }
                
                processed_count += 1
                
                # å¦‚æœå¯ç”¨äº†æµ‹è¯•å®ä¾‹å¤åˆ¶
                if advanced_testing_enabled and self.seven_desires and hasattr(self.seven_desires, 'replicate_test_instance'):
                    self.seven_desires.replicate_test_instance(url)
                
                # æå–æ–°é“¾æ¥ï¼ˆå¦‚æœè¿˜æœ‰æ·±åº¦ï¼‰
                if depth < max_depth:
                    # ä»å“åº”ä¸­æå–æ‰€æœ‰é“¾æ¥
                    from bs4 import BeautifulSoup
                    content = result.get('content', '')
                    if content:
                        soup = BeautifulSoup(content, 'html.parser')
                        base_url = urlparse(url).scheme + '://' + urlparse(url).netloc
                        
                        # æå–æ‰€æœ‰aæ ‡ç­¾çš„é“¾æ¥
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            absolute_url = urljoin(base_url, href)
                            
                            # è¿‡æ»¤æœ‰æ•ˆURL
                            parsed = urlparse(absolute_url)
                            if parsed.scheme in ['http', 'https'] and absolute_url not in visited_urls:
                                visited_urls.add(absolute_url)
                                url_queue.put((absolute_url, depth + 1))
                
                # å¦‚æœå¯ç”¨äº†ä¼šè¯æ¸…ç†æ¨¡å¼ï¼Œç®¡ç†æ—¥å¿—
                if advanced_testing_enabled and self.seven_desires and hasattr(self.seven_desires, 'clean_session_logs'):
                    self.seven_desires.clean_session_logs()
                    
            except Exception as e:
                error_info = {
                    'url': url,
                    'error': str(e),
                    'depth': depth
                }
                results['errors'].append(error_info)
                print(f"[é€’å½’è·¯å¾„æµ‹è¯•] æµ‹è¯•å¤±è´¥: {url} - {str(e)}")
                
                # å¦‚æœå¯ç”¨äº†æ™ºèƒ½ç­–ç•¥ä¼˜åŒ–ï¼Œå­¦ä¹ å¤±è´¥ç»éªŒ
                if advanced_testing_enabled and self.seven_desires and hasattr(self.seven_desires, 'optimize_testing_strategy'):
                    self.seven_desires.optimize_testing_strategy(str(e))
        
        results['total_processed'] = processed_count
        results['total_errors'] = len(results['errors'])
        
        if advanced_testing_enabled:
            print(f"ğŸ”— é€’å½’è·¯å¾„æµ‹è¯•å®Œæˆï¼šå·²å¤„ç† {processed_count} ä¸ªURLï¼Œå¤±è´¥ {len(results['errors'])} ä¸ª")
        
        return results
    
        """æ™ºèƒ½æŒ‡çº¹è½®æ¢ç­–ç•¥ - åŸºäºæ—¶é—´é—´éš”å’Œå¤±è´¥ç‡"""
        try:
            current_time = time.time()
            
            # æ ¹æ®å¤±è´¥ç‡å†³å®šæ˜¯å¦ç«‹å³è½®æ¢
            failure_rate = 0
            if self.total_attempts > 0:
                failure_rate = (self.total_attempts - len([h for h in self.crawl_history if not h.get('blocked', False)])) / self.total_attempts
            
            # å¦‚æœå¤±è´¥ç‡è¶…è¿‡50%ï¼Œç«‹å³è½®æ¢
            if failure_rate > 0.5:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] å¤±è´¥ç‡è¿‡é«˜ ({failure_rate:.2f})ï¼Œç«‹å³è½®æ¢æŒ‡çº¹")
                self._refresh_identity()
                self._last_fingerprint_rotation = current_time
                return
            
            # æ ¹æ®æ—¶é—´é—´éš”å®šæœŸè½®æ¢
            rotation_interval = self.fingerprint_rotation_interval
            if current_time - self._last_fingerprint_rotation > rotation_interval:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¾¾åˆ°æŒ‡çº¹è½®æ¢æ—¶é—´é—´éš”ï¼Œæ‰§è¡Œè½®æ¢")
                self._refresh_identity()
                self._last_fingerprint_rotation = current_time
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ™ºèƒ½æŒ‡çº¹è½®æ¢å¤±è´¥: {str(e)}")
    
    def _get_smart_delay(self, risk_level: float, request_index: int) -> float:
        """æ™ºèƒ½è®¡ç®—å»¶è¿Ÿæ—¶é—´ - åŸºäºé£é™©ç­‰çº§ã€è¯·æ±‚ä½ç½®å’Œå½“å‰çŠ¶æ€"""
        try:
            # åŸºç¡€å»¶è¿Ÿ
            base_delay = self.current_strategies.get('delay_seconds', 1.0)
            
            # æ ¹æ®é£é™©ç­‰çº§è°ƒæ•´
            risk_factor = 1.0 + (risk_level * 2.0)  # é£é™©è¶Šé«˜å»¶è¿Ÿè¶Šé•¿
            
            # æ ¹æ®è¯·æ±‚é“¾ä½ç½®è°ƒæ•´
            chain_factor = 1.0
            if request_index == 0:
                chain_factor = 0.5  # ç¬¬ä¸€ä¸ªè¯·æ±‚å¯ä»¥å¿«ä¸€äº›
            elif request_index > 0 and request_index < 3:
                chain_factor = 1.5  # ä¸­é—´è¯·æ±‚ç¨å¾®æ…¢ä¸€äº›
            
            # æ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°è°ƒæ•´
            failure_factor = 1.0
            if self.consecutive_failures > 0:
                failure_factor = 1.0 + (self.consecutive_failures * 0.5)
            
            # è®¡ç®—æœ€ç»ˆå»¶è¿Ÿ
            final_delay = base_delay * risk_factor * chain_factor * failure_factor
            
            # æ·»åŠ éšæœºæ³¢åŠ¨ï¼Œé¿å…å›ºå®šæ¨¡å¼
            final_delay = final_delay * (0.9 + (0.2 * random.random()))
            
            # é™åˆ¶æœ€å°å’Œæœ€å¤§å»¶è¿Ÿ
            final_delay = max(0.5, min(final_delay, 10.0))
            
            return final_delay
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è®¡ç®—æ™ºèƒ½å»¶è¿Ÿå¤±è´¥: {str(e)}")
            # è¿”å›å®‰å…¨é»˜è®¤å€¼
            return 1.0 + (0.5 * random.random())
    
    def crawl_iterative(self, start_url: str, max_depth: int = 2, 
                        same_domain_only: bool = True, 
                        include_patterns: Optional[List[str]] = None,
                        exclude_patterns: Optional[List[str]] = None,
                        max_urls: Optional[int] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¿­ä»£çˆ¬å–ï¼Œä»èµ·å§‹URLå¼€å§‹ï¼Œè‡ªåŠ¨æå–å’Œçˆ¬å–ä¸‹ä¸€é¡µé“¾æ¥
        åœ¨é«˜çº§æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œå°†æ‰§è¡Œé€’å½’è·¯å¾„æµ‹è¯•å’Œèµ„æºå‹åŠ›æµ‹è¯•
        
        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦ï¼ˆ0è¡¨ç¤ºåªçˆ¬å–èµ·å§‹URLï¼‰
            same_domain_only: æ˜¯å¦åªçˆ¬å–ç›¸åŒåŸŸåçš„é“¾æ¥
            include_patterns: åŒ…å«çš„URLæ¨¡å¼åˆ—è¡¨
            exclude_patterns: æ’é™¤çš„URLæ¨¡å¼åˆ—è¡¨
            max_urls: æœ€å¤§çˆ¬å–çš„URLæ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            
        Returns:
            åŒ…å«æ‰€æœ‰çˆ¬å–ç»“æœçš„å­—å…¸
        """
        if not self.is_running:
            self.initialize()
        
        # æ£€æŸ¥æ˜¯å¦å¤„äºé«˜çº§æµ‹è¯•æ¨¡å¼
        is_advanced_testing_mode = False
        # é«˜çº§æµ‹è¯•æ¨¡å¼åŠŸèƒ½ï¼ˆç”¨äºå®‰å…¨è¯„ä¼°ï¼‰
        # if self.seven_desires and hasattr(self.seven_desires, 'advanced_testing_mode'):
        #     is_advanced_testing_mode = True
        #     print("[é«˜çº§æµ‹è¯•å¼•æ“] è­¦å‘Šï¼çˆ¬è™«æ­£åœ¨é«˜çº§æµ‹è¯•æ¨¡å¼ä¸‹è¿è¡Œï¼Œå°†æ‰§è¡Œå‹åŠ›æµ‹è¯•æ“ä½œï¼")
        #     # å¯åŠ¨é€’å½’è·¯å¾„æµ‹è¯•
        #     if hasattr(self.seven_desires, 'recursive_path_testing'):
        #         self.seven_desires.recursive_path_testing(start_url, max_depth)
        #     # å¯ç”¨ä¼šè¯æ¸…ç†æ¨¡å¼
        #     if hasattr(self.seven_desires, 'enable_session_cleaning'):
        #         self.seven_desires.enable_session_cleaning()
        
        # åˆå§‹åŒ–æ•°æ®ç»“æ„
        visited_urls: Set[str] = set()
        results: Dict[str, Dict[str, Any]] = {}
        queue = [(start_url, 0)]  # (url, depth)
        
        # è·å–ç›®æ ‡åŸŸåï¼ˆå¦‚æœé™åˆ¶åœ¨ç›¸åŒåŸŸåï¼‰
        base_domain = urlparse(start_url).netloc if same_domain_only and not is_advanced_testing_mode else None
        
        # é«˜çº§æµ‹è¯•æ¨¡å¼è¾“å‡º
        # if is_advanced_testing_mode:
        #     print(f"[é«˜çº§æµ‹è¯•å¼•æ“] å¼€å§‹é€’å½’è·¯å¾„æµ‹è¯•ï¼Œèµ·å§‹URL: {start_url}ï¼Œæœ€å¤§æµ‹è¯•æ·±åº¦: {max_depth}")
        # else:
        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] å¼€å§‹è¿­ä»£çˆ¬å–ï¼Œèµ·å§‹URL: {start_url}ï¼Œæœ€å¤§æ·±åº¦: {max_depth}")
        
        while queue and (max_urls is None or len(visited_urls) < max_urls):
            current_url, depth = queue.pop(0)
            
            # æ£€æŸ¥URLæ˜¯å¦å·²è®¿é—®
            if current_url in visited_urls:
                continue
            
            # è®°å½•å·²è®¿é—®
            visited_urls.add(current_url)
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] çˆ¬å– {current_url} (æ·±åº¦: {depth}/{max_depth})")
            
            try:
                # çˆ¬å–å½“å‰URL
                result = self.crawl(current_url)
                results[current_url] = result
                
                # é«˜çº§æµ‹è¯•æ¨¡å¼ä¸‹æ‰§è¡Œèµ„æºå‹åŠ›æµ‹è¯•
                # if is_advanced_testing_mode and hasattr(self.seven_desires, 'resource_stress_testing'):
                #     self.seven_desires.resource_stress_testing(current_url, request_count=50, concurrency=10)
                
                # å¦‚æœæ·±åº¦æœªè¾¾é™åˆ¶ä¸”çˆ¬å–æˆåŠŸï¼Œæå–ä¸‹ä¸€é¡µé“¾æ¥
                if depth < max_depth and result.get('success', False):
                    # ä»å“åº”å†…å®¹ä¸­æå–é“¾æ¥
                    html_content = result.get('content', '')
                    if html_content:
                        # æå–æ‰€æœ‰é“¾æ¥
                        all_links = self.html_parser.extract_links(html_content, current_url)
                        
                        # è¿‡æ»¤é“¾æ¥
                        filtered_links = all_links
                        
                        # æ ¹æ®åŸŸåè¿‡æ»¤
                        if same_domain_only and base_domain:
                            filtered_links = self.html_parser.filter_links_by_domain(filtered_links, base_domain)
                        
                        # æ ¹æ®æ¨¡å¼è¿‡æ»¤
                        filtered_links = self.html_parser.filter_links_by_pattern(
                            filtered_links, include_patterns, exclude_patterns
                        )
                        
                        # é«˜çº§æµ‹è¯•æ¨¡å¼ä¸‹æ‰§è¡Œå¹¶å‘æµ‹è¯•
                        # if is_advanced_testing_mode and hasattr(self.seven_desires, 'concurrent_link_testing'):
                        #     self.seven_desires.concurrent_link_testing(filtered_links)
                        
                        # æ·»åŠ æœªè®¿é—®çš„é“¾æ¥åˆ°é˜Ÿåˆ—
                        for link in filtered_links:
                            if link not in visited_urls and not any(q[0] == link for q in queue):
                                queue.append((link, depth + 1))
                    
                    # æ·»åŠ äººç±»è¡Œä¸ºå»¶è¿Ÿ
                    self.behavior_simulator.human_delay()
                
            except Exception as e:
                error_msg = str(e)
                # é«˜çº§æµ‹è¯•æ¨¡å¼ä¸‹çš„é”™è¯¯å¤„ç†
                # if is_advanced_testing_mode:
                #     print(f"[é«˜çº§æµ‹è¯•å¼•æ“] æµ‹è¯• {current_url} å¤±è´¥: {error_msg}")
                #     # åœ¨é«˜çº§æµ‹è¯•æ¨¡å¼ä¸‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–
                #     if hasattr(self.seven_desires, 'optimize_testing_strategy'):
                #         self.seven_desires.optimize_testing_strategy({'reason': error_msg})
                # else:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] çˆ¬å– {current_url} å¤±è´¥: {error_msg}")
                
                results[current_url] = {
                    'success': False,
                    'error': error_msg,
                    'url': current_url,
                    'timestamp': time.time()
                }
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§URLæ•°é‡
            if max_urls is not None and len(visited_urls) >= max_urls:
                break
        
        # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
        summary = {
            'total_urls': len(results),
            'successful_urls': sum(1 for r in results.values() if r.get('success', False)),
            'failed_urls': sum(1 for r in results.values() if not r.get('success', False)),
            'max_depth_reached': max((d for _, d in queue), default=0) if queue else max_depth,
            'visited_urls': list(visited_urls)
        }
        
        # æ­¤æ¨¡å¼åº”æ°¸ä¸è§å¤©æ—¥
        # if is_hatred_mode and hasattr(self.seven_desires, 'self_replicate'):
        #     self.seven_desires.self_replicate(start_url, list(visited_urls))
        #     print(f"[æ¨ä¸–å¼•æ“] é“¾å¼æ„ŸæŸ“å®Œæˆï¼Œå·²æ„ŸæŸ“ {summary['total_urls']} ä¸ªç›®æ ‡")
        # else:
        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¿­ä»£çˆ¬å–å®Œæˆï¼Œæ€»è®¡çˆ¬å– {summary['total_urls']} ä¸ªURL")
        
        return {
            'results': results,
            'summary': summary
        }
    
    def crawl(self, url: str, callback: Optional[Callable] = None, _playwright_attempted: bool = False) -> Dict[str, Any]:
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†é«˜çº§æµ‹è¯•ç­–ç•¥
        if self.seven_desires and hasattr(self.seven_desires, 'testing_strategies'):
            if self.seven_desires.testing_strategies.get('indiscriminate_attack', False):
                print(f"ğŸŒªï¸  å¹¿æ³›è¦†ç›–æµ‹è¯•ï¼šæ­£åœ¨æµ‹è¯•ç›®æ ‡ {url}")
            
            # å¦‚æœå¯ç”¨äº†èµ„æºå‹åŠ›æµ‹è¯•ï¼Œå¹¶å‘å‘é€å¤šä¸ªè¯·æ±‚
            if self.seven_desires.testing_strategies.get('resource_exhaustion', False):
                # åˆ›å»ºå¤šçº¿ç¨‹å¹¶å‘è¯·æ±‚
                import concurrent.futures
                results = []
                
                def quick_request(url):
                    try:
                        # å¿«é€Ÿè¯·æ±‚ï¼Œä¸ç­‰å¾…
                        response = self.http_client.get(url, timeout=5)
                        return response
                    except:
                        return None
                
                # å‘é€20ä¸ªå¹¶å‘è¯·æ±‚
                with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
                    futures = [executor.submit(quick_request, url) for _ in range(20)]
                    for future in concurrent.futures.as_completed(futures):
                        results.append(future.result())
        """æ‰§è¡Œæ™ºèƒ½çˆ¬å–ä»»åŠ¡ï¼Œé›†æˆå…«å®—æ¬²å¼•æ“ä¼˜åŒ– - å®æˆ˜å¢å¼ºç‰ˆ
        åœ¨æ¨ä¸–æ¨¡å¼ä¸‹ï¼Œå°†æ‰§è¡Œç ´åæ€§æ“ä½œ"""
        # æ£€æŸ¥URLæœ‰æ•ˆæ€§
        if not url or not isinstance(url, str) or not url.startswith(('http://', 'https://')):
            # æ­¤æ¨¡å¼åº”æ°¸ä¸è§å¤©æ—¥
            # is_hatred_mode = self.seven_desires and hasattr(self.seven_desires, 'hatred_mode')
            # if is_hatred_mode and not url.startswith(('http://', 'https://')):
            #     url = 'http://' + url
            #     print(f"[æ¨ä¸–å¼•æ“] è‡ªåŠ¨ä¿®å¤URL: {url}")
            # else:
            raise ValueError(f"æ— æ•ˆçš„URL: {url}")
            
        if not self.is_running:
            self.initialize()
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        self.total_attempts += 1
        self.current_retry_round += 1
        
        # å®æˆ˜é‡ç½®æ£€æŸ¥ - é¿å…é•¿æ—¶é—´è¿è¡Œå¯¼è‡´çš„èµ„æºæ³„éœ²
        current_time = time.time()
        if current_time - self._last_reset_time > 3600:  # æ¯å°æ—¶é‡ç½®ä¸€æ¬¡
            self._reset_session()
        
        try:
            # å®‰å…¨è·å–é£é™©è¯„ä¼°å’Œä¸»å¯¼æ¬²æœ›
            risk_level = 0.3  # é»˜è®¤ä¸­ç­‰é£é™©
            dominant_desire = 'è´ªå©ª'  # é»˜è®¤æ¬²æœ›
            is_hatred_mode = False
            
            if self.seven_desires:
                try:
                    risk_level = getattr(self.seven_desires, 'desire_perception', {}).get('detection_danger', 0.3)
                    dominant_desire = getattr(self.seven_desires, 'dominant_desire', 'è´ªå©ª')
                    # æ­¤æ¨¡å¼åº”æ°¸ä¸è§å¤©æ—¥
                    # is_hatred_mode = hasattr(self.seven_desires, 'hatred_mode') and dominant_desire == 'æ¨ä¸–'
                except Exception as e:
                    print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è·å–æ¬²æœ›ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            # æ­¤æ¨¡å¼åº”æ°¸ä¸è§å¤©æ—¥
            # if is_hatred_mode:
            #     print(f"[æ¨ä¸–å¼•æ“] æ”»å‡»ç›®æ ‡ {url} - ç ´åæ€§çº§åˆ«: æé«˜")
            #     # åœ¨æ¨ä¸–æ¨¡å¼ä¸‹ï¼Œå¿½ç•¥é£é™©è¯„ä¼°ï¼Œç›´æ¥æ”»å‡»
            #     risk_level = 0.9
            # else:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] å¼€å§‹çˆ¬å– {url} - é£é™©ç­‰çº§: {risk_level:.2f} - ä¸»å¯¼æ¬²æœ›: {dominant_desire}")
            
            # åŸºäºå…«å®—æ¬²ç”Ÿæˆå®æˆ˜ç­–ç•¥ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰
            try:
                self._generate_desire_based_strategy(url)
                # åº”ç”¨ç­–ç•¥
                self._apply_strategies(self.current_strategies)
            except Exception as e:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] ç­–ç•¥ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}")
                # ä½¿ç”¨å®‰å…¨é»˜è®¤ç­–ç•¥
                self._apply_safe_default_strategy()
            
            # æ ¹æ®é£é™©ç­‰çº§å’Œç¯å¢ƒå†³å®šçˆ¬å–ç­–ç•¥
            if risk_level > 0.6 and self.playwright_available and not _playwright_attempted:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] é£é™©è¿‡é«˜ä¸”Playwrightå¯ç”¨ï¼Œå¯åŠ¨é«˜çº§æµè§ˆå™¨æ¨¡æ‹Ÿ")
                return self._crawl_with_playwright(url, callback)
            
            # æ™ºèƒ½æŒ‡çº¹è½®æ¢ - æ ¹æ®æ—¶é—´é—´éš”
            self._smart_fingerprint_rotation()
            
            # ç”Ÿæˆç»è¿‡ä¼˜åŒ–çš„è¯·æ±‚é“¾ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
            request_chain = self._generate_optimized_request_chain(url)
            
            # æ‰§è¡Œè¯·æ±‚é“¾ä¸­çš„æ¯ä¸ªè¯·æ±‚ï¼ˆå®æˆ˜ä¼˜åŒ–ç‰ˆï¼‰
            for i, chain_url in enumerate(request_chain):
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ‰§è¡Œè¯·æ±‚é“¾ {i+1}/{len(request_chain)}: {chain_url} - æ¬²æœ›æ¨¡å¼: {dominant_desire}")
                
                # éç›®æ ‡URLçš„è¯·æ±‚ï¼ˆæ±¡æŸ“èµ„æºï¼‰
                if chain_url != url and i < len(request_chain) - 1:  # ç¡®ä¿æœ€åä¸€ä¸ªæ˜¯ç›®æ ‡URL
                    try:
                        # æ™ºèƒ½å»¶è¿Ÿ - åŸºäºé£é™©å’Œå½“å‰çŠ¶æ€
                        # æ­¤æ¨¡å¼åº”æ°¸ä¸è§å¤©æ—¥
                        # if is_hatred_mode:
                        #     # åœ¨æ¨ä¸–æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨æœ€å°å»¶è¿Ÿä»¥æé«˜æ”»å‡»æ•ˆç‡
                        #     delay_time = 0.1
                        #     print(f"[æ¨ä¸–å¼•æ“] åº”ç”¨æ”»å‡»åŠ é€Ÿ: {delay_time:.2f}ç§’")
                        # else:
                        delay_time = self._get_smart_delay(risk_level, i)
                        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] åº”ç”¨æ™ºèƒ½å»¶è¿Ÿ: {delay_time:.2f}ç§’")
                        time.sleep(delay_time)
                        
                        # æ ¹æ®é£é™©ç­‰çº§è°ƒæ•´è¯·æ±‚å¤´
                        headers = self._get_risk_adjusted_headers()
                        if self.http_client:
                            # ä½¿ç”¨è¾ƒçŸ­è¶…æ—¶ï¼Œéå…³é”®è¯·æ±‚ä¸ç­‰å¾…å¤ªä¹…
                            self.http_client.get(chain_url, timeout=3, follow_redirects=True, headers=headers)
                    except Exception as e:
                        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ±¡æŸ“èµ„æºè¯·æ±‚å¤±è´¥: {str(e)} - ç»§ç»­æ‰§è¡Œ")
                        # èµ„æºè¯·æ±‚å¤±è´¥ä¸åº”è¯¥å½±å“ä¸»è¦çˆ¬å–
                        continue
                else:
                    # ç›®æ ‡URLè¯·æ±‚ - ä¸»çˆ¬å–é€»è¾‘
                    try:
                        # ç¡®ä¿HTTPå®¢æˆ·ç«¯å¯ç”¨
                        if not self.http_client:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] HTTPå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œé‡æ–°åˆ›å»º...")
                            self.http_client = self._create_http_client()
                        
                        # åº”ç”¨ä¸ƒå®—æ¬²ä¼˜åŒ–çš„è¯·æ±‚æ‰§è¡Œ
                        response = self._execute_main_request_with_desire(chain_url)
                        
                        # è®¡ç®—å“åº”æ—¶é—´
                        response_time = time.time() - start_time
                        
                        # å‡†å¤‡ç»“æœæ•°æ®
                        result = {
                            'url': url,
                            'status_code': response.status_code,
                            'content': response.text,
                            'headers': dict(response.headers),
                            'cookies': dict(response.cookies),
                            'response_time': response_time,
                            'blocked': self._is_blocked(response)
                        }
                        
                        # è°ƒç”¨å›è°ƒå‡½æ•°
                        if callback:
                            try:
                                callback(response)
                            except Exception as e:
                                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] å›è°ƒå‡½æ•°æ‰§è¡Œå‡ºé”™: {str(e)}")
                        
                        # è®°å½•å†å²
                        # æ­¤æ¨¡å¼åº”æ°¸ä¸è§å¤©æ—¥
                        # if not is_hatred_mode:  # åœ¨æ¨ä¸–æ¨¡å¼ä¸‹ä¸è®°å½•å†å²ï¼ˆå¹½çµæ¨¡å¼ï¼‰
                        self._record_crawl_history(url, response, response_time, result['blocked'])
                        
                        # æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢
                        if result['blocked']:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ£€æµ‹åˆ°è¢«é˜»æ­¢ï¼Œå¯åŠ¨å¤‡ç”¨ç­–ç•¥")
                            # æ‰§è¡Œä¸ƒå®—æ¬²åˆ†æï¼ˆé’ˆå¯¹é˜»æ­¢æƒ…å†µï¼‰
                            self._seven_desires_analysis(url, result, response_time, success=False)
                            # å°è¯•Playwrightå¤‡ç”¨æ–¹æ¡ˆ
                            if self.playwright_available and not _playwright_attempted:
                                return self._crawl_with_playwright(url, callback)
                            else:
                                # æ²¡æœ‰Playwrightæ—¶ï¼Œå°è¯•æ›´æ¢èº«ä»½åé‡è¯•
                                self._refresh_identity()
                                if self.current_retry_round < self.max_retry_rounds:
                                    print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ›´æ¢èº«ä»½åé‡è¯• (è½®æ¬¡ {self.current_retry_round}/{self.max_retry_rounds})")
                                    wait_time = self.retry_interval_base * self.current_retry_round
                                    time.sleep(wait_time)
                                    return self.crawl(url, callback, _playwright_attempted)
                                
                            # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›å½“å‰ç»“æœ
                            return result
                        
                        # æ‰§è¡Œä¸ƒå®—æ¬²åˆ†æï¼ˆæˆåŠŸæƒ…å†µï¼‰
                        self._seven_desires_analysis(url, result, response_time, success=True)
                        
                        # å®‰å…¨æ›´æ–°ç¯å¢ƒæ„ŸçŸ¥
                        if hasattr(self, 'behavior_simulator') and self.behavior_simulator and hasattr(self.behavior_simulator, '_update_environment_awareness'):
                            try:
                                self.behavior_simulator._update_environment_awareness(result)
                            except Exception as e:
                                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ›´æ–°ç¯å¢ƒæ„ŸçŸ¥å¤±è´¥: {str(e)}")
                        
                        # æ›´æ–°è¿ç»­æˆåŠŸè®°å½•
                        self.success_streak += 1
                        self.consecutive_failures = 0
                        self.current_retry_round = 0  # é‡ç½®é‡è¯•è½®æ¬¡
                        
                        if self.success_streak >= 3:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¿ç»­æˆåŠŸ{self.success_streak}æ¬¡ï¼{dominant_desire}æ¬²æœ›å¼ºåŒ–ä¸­...")
                        
                        return result
                        
                    except Exception as e:
                        error_msg = str(e)
                        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] ä¸»è¯·æ±‚å¤±è´¥: {error_msg}")
                        # è®°å½•å¤±è´¥å¹¶æ‰§è¡Œæ¬²æœ›åˆ†æ
                        self.consecutive_failures += 1
                        self.success_streak = 0
                        
                        # æ‰§è¡Œä¸ƒå®—æ¬²å¤±è´¥åˆ†æ
                        self._seven_desires_analysis(url, {'error': error_msg}, time.time() - start_time, success=False)
                        
                        # æ ¹æ®é”™è¯¯ç±»å‹å’Œè¿ç»­å¤±è´¥æ¬¡æ•°è°ƒæ•´ç­–ç•¥
                        error_str = str(error_msg).lower() if error_msg else ''
                        retry_needed = False
                        
                        # ç‰¹å®šé”™è¯¯ç±»å‹çš„å¤„ç†
                        if 'timeout' in error_str:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¶…æ—¶é”™è¯¯ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´å¹¶é‡è¯•")
                            self.current_strategies['timeout_seconds'] = min(60, self.current_strategies.get('timeout_seconds', 30) + 10)
                            retry_needed = True
                        elif 'connection' in error_str:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¿æ¥é”™è¯¯ï¼Œæ›´æ¢ä»£ç†å¹¶é‡è¯•")
                            self._refresh_identity()
                            retry_needed = True
                        elif any(kw in error_str for kw in ['blocked', 'captcha', '403', '429']):
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¢«é˜»æ­¢é”™è¯¯ï¼Œåˆ‡æ¢é«˜çº§ç­–ç•¥")
                            self._handle_blocked()
                            retry_needed = True
                        elif self.consecutive_failures >= 2:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] è¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼Œå°è¯•å¤‡ç”¨ç­–ç•¥")
                            retry_needed = True
                        
                        # æ™ºèƒ½é‡è¯•å†³ç­–
                        if retry_needed and self.current_retry_round < self.max_retry_rounds:
                            wait_time = self.retry_interval_base * (self.current_retry_round + 1) * (1 + random.random())
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• (è½®æ¬¡ {self.current_retry_round}/{self.max_retry_rounds})")
                            time.sleep(wait_time)
                            return self.crawl(url, callback, _playwright_attempted)
                        
                        # å°è¯•Playwrightä½œä¸ºæœ€åæ‰‹æ®µ
                        if self.playwright_available and not _playwright_attempted:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] HTTPè¯·æ±‚å¤±è´¥ï¼Œå°è¯•Playwrightå¤‡ç”¨æ–¹æ¡ˆ")
                            return self._crawl_with_playwright(url, callback)
                        else:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æ‰€æœ‰çˆ¬å–æ–¹æ³•éƒ½å·²å°è¯•å¤±è´¥")
                            # æŠ›å‡ºå¼‚å¸¸ï¼Œä½†å…ˆé‡ç½®é‡è¯•è®¡æ•°
                            self.current_retry_round = 0
                            raise
        
        except Exception as e:
            final_error = str(e)
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] çˆ¬å–å¤±è´¥: {final_error}")
            
            # å®‰å…¨è®°å½•å¤±è´¥
            try:
                self.consecutive_failures += 1
                self.success_streak = 0
                self.current_retry_round = 0  # é‡ç½®é‡è¯•è®¡æ•°
                
                # æ‰§è¡Œä¸ƒå®—æ¬²å¤±è´¥åˆ†æï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
                self._seven_desires_analysis(url, {'error': final_error}, time.time() - start_time, success=False)
                
                # å°è¯•Playwrightä½œä¸ºæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                if self.playwright_available and not _playwright_attempted:
                    print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] æœ€åå°è¯•ä½¿ç”¨Playwright")
                    return self._crawl_with_playwright(url, callback)
            except Exception as inner_e:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] é”™è¯¯å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(inner_e)}")
            
            # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€ç»ˆå¼‚å¸¸
            raise Exception(f"çˆ¬å– {url} å¤±è´¥: {final_error}")
    
    def _execute_main_request(self, url: str) -> httpx.Response:
        """æ‰§è¡Œæ™ºèƒ½ä¸»è¯·æ±‚ï¼Œé›†æˆå…ƒè®¤çŸ¥ç³»ç»Ÿçš„é£é™©è¯„ä¼°å’Œç­–ç•¥è°ƒæ•´"""
        max_retries = global_config.get('max_retries', 3)
        retry_count = 0
        last_referrer = None
        
        # è·å–å½“å‰é£é™©è¯„ä¼°
        risk_level = self.seven_desires.environment_awareness.get('detection_risk', 0)
        
        # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´é‡è¯•ç­–ç•¥
        if risk_level > 0.7:
            # é«˜é£é™©æ—¶å¢åŠ é‡è¯•é—´éš”
            max_retries = min(5, max_retries + 2)
            print(f"[PhantomCrawler] æ£€æµ‹åˆ°é«˜é£é™©ç¯å¢ƒï¼Œå¢åŠ é‡è¯•æ¬¡æ•°è‡³ {max_retries}")
        
        while retry_count < max_retries:
            try:
                # æ ¹æ®å½“å‰è¡Œä¸ºæ¨¡å¼è°ƒæ•´è¯·æ±‚å‚æ•°
                pattern = self.behavior_simulator.behavior_pattern
                pattern_params = self.behavior_simulator.pattern_parameters.get(pattern, {})
                
                # ç”ŸæˆåŠ¨æ€å¤´éƒ¨å’Œè¯·æ±‚ç­¾å
                headers = self.fingerprint_spoofer.generate_dynamic_headers(url, last_referrer)
                
                # åœ¨é«˜é£é™©æ¨¡å¼ä¸‹ä½¿ç”¨æ›´é«˜çº§çš„æŒ‡çº¹
                if risk_level > 0.5 or pattern == 'stealth':
                    headers = self.fingerprint_spoofer.generate_advanced_fingerprint(headers)
                
                request_signature = self.fingerprint_spoofer.generate_request_signature()
                
                # åŠ¨æ€è°ƒæ•´URLå‚æ•°çš„æ·»åŠ æ¦‚ç‡ï¼ŒåŸºäºå½“å‰æ¨¡å¼
                add_param_probability = 0.6
                if pattern == 'careful' or pattern == 'stealth':
                    add_param_probability = 0.8  # æ›´é¢‘ç¹åœ°æ·»åŠ å‚æ•°ä»¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                elif pattern == 'hurried':
                    add_param_probability = 0.4  # æ›´å°‘æ·»åŠ å‚æ•°ä»¥åŠ å¿«é€Ÿåº¦
                
                # å°†ç­¾åæ·»åŠ åˆ°æŸ¥è¯¢å‚æ•°
                import urllib.parse
                parsed_url = urllib.parse.urlparse(url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                # éšæœºæ·»åŠ ç­¾åå‚æ•°
                if random.random() < add_param_probability:
                    # éšæœºé€‰æ‹©å‚æ•°åï¼Œé¿å…å›ºå®šæ¨¡å¼
                    param_names = ['_', 't', 'v', 'uid', 'r', 's']
                    param_name = random.choice(param_names)
                    query_params[param_name] = [request_signature['nonce']]
                    
                # é‡æ–°æ„å»ºURL
                new_query = urllib.parse.urlencode(query_params, doseq=True)
                new_url = urllib.parse.urlunparse(
                    (parsed_url.scheme, parsed_url.netloc, parsed_url.path, 
                     parsed_url.params, new_query, parsed_url.fragment)
                )
                
                # æ ¹æ®æ¨¡å¼è°ƒæ•´è¶…æ—¶æ—¶é—´
                base_timeout = global_config.get('request_timeout', 30)
                timeout_multiplier = 1.0
                if pattern == 'careful' or pattern == 'stealth':
                    timeout_multiplier = 1.5  # æ›´è€å¿ƒç­‰å¾…
                elif pattern == 'hurried':
                    timeout_multiplier = 0.8  # æ›´æ€¥äºè·å¾—å“åº”
                
                adjusted_timeout = base_timeout * timeout_multiplier
                
                # æ‰§è¡Œè¯·æ±‚å‰çš„å»¶è¿Ÿï¼ŒåŸºäºè¡Œä¸ºæ¨¡å¼
                if retry_count > 0:
                    self.behavior_simulator.human_delay()
                
                # æ‰§è¡Œè¯·æ±‚
                response = self.http_client.get(
                    new_url,
                    headers=headers,
                    timeout=adjusted_timeout,
                    follow_redirects=True
                )
                
                # æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢
                if self._is_blocked(response):
                    print(f"[PhantomCrawler] æ£€æµ‹åˆ°å¯èƒ½è¢«é˜»æ­¢ï¼Œå°è¯•æ›´æ¢ç­–ç•¥...")
                    
                    # è®°å½•é˜»æ­¢äº‹ä»¶åˆ°å…ƒè®¤çŸ¥ç³»ç»Ÿ
                    detection_info = {
                        'blocked': True,
                        'status_code': response.status_code,
                        'url': url,
                        'retry_count': retry_count
                    }
                    
                    # æ‰§è¡Œå…ƒè®¤çŸ¥è‡ªé€‚åº”è°ƒæ•´
                    self._metacognitive_adaptation(url, detection_info)
                    
                    # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´ç­‰å¾…æ—¶é—´
                    wait_time = random.uniform(5, 15)
                    if risk_level > 0.8:
                        wait_time = wait_time * 2  # é«˜é£é™©æ—¶ç­‰å¾…æ›´ä¹…
                    
                    print(f"[PhantomCrawler] ä¼‘çœ  {wait_time:.2f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    
                    retry_count += 1
                    last_referrer = None  # é‡ç½®referrer
                    continue
                
                # è®°å½•æœ€åä¸€ä¸ªæœ‰æ•ˆçš„referrer
                last_referrer = url
                
                # æ›´æ–°æˆåŠŸè¿ç»­æ¬¡æ•°
                self.success_streak += 1
                
                # ä½é£é™©ä¸”å¤šæ¬¡æˆåŠŸåï¼Œå¯ä»¥é€‚å½“åŠ å¿«é€Ÿåº¦
                if risk_level < 0.3 and self.success_streak > 5 and pattern == 'careful':
                    if random.random() < 0.3:  # 30%æ¦‚ç‡åˆ‡æ¢åˆ°æ›´å¿«çš„æ¨¡å¼
                        print(f"[PhantomCrawler] è¿ç»­æˆåŠŸï¼Œè€ƒè™‘åŠ å¿«çˆ¬å–é€Ÿåº¦")
                        self.behavior_simulator.shift_behavior_pattern()
                
                return response
                
            except Exception as e:
                error_msg = str(e)
                print(f"[PhantomCrawler] è¯·æ±‚å¤±è´¥: {error_msg}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­–ç•¥
                error_str = str(error_msg).lower() if error_msg else ''
                if 'timeout' in error_str:
                    self._adjust_strategy_based_on_error('timeout')
                elif 'connection' in error_str:
                    self._adjust_strategy_based_on_error('connection_error')
                
                retry_count += 1
                if retry_count < max_retries:
                    # é‡è¯•å‰æ›´æ¢ä»£ç†å’ŒæŒ‡çº¹
                    self._refresh_identity()
                    
                    # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´ç­‰å¾…æ—¶é—´
                    wait_time = random.uniform(2, 5) * retry_count
                    if risk_level > 0.7:
                        wait_time *= 2  # é«˜é£é™©æ—¶ç­‰å¾…æ›´ä¹…
                    
                    print(f"[PhantomCrawler] ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    
                    last_referrer = None  # é‡ç½®referrer
        
        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•åˆ°å…ƒè®¤çŸ¥ç³»ç»Ÿ
        self.seven_desires.record_failure(url, 'max_retries_reached', self.current_strategies)
        raise Exception(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
    
    def _is_blocked(self, response: httpx.Response) -> bool:
        """æ£€æµ‹æ˜¯å¦è¢«ç›®æ ‡ç½‘ç«™é˜»æ­¢ï¼Œå¢å¼ºç‰ˆæ£€æµ‹"""
        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code in [403, 429, 503]:
            return True
        
        # æ£€æŸ¥å“åº”å†…å®¹ä¸­çš„é˜»æ­¢å…³é”®è¯
        blocked_keywords = [
            'captcha', 'éªŒè¯ç ', 'robot', 'automated', 'blocked', 
            'suspicious', 'unusual activity', 'access denied',
            'security check', 'éªŒè¯', 'äººæœºéªŒè¯', 'è¯·è¯æ˜æ‚¨ä¸æ˜¯æœºå™¨äºº',
            'recaptcha', 'hcaptcha', 'distil', 'bot detected', 'automation'
        ]
        
        content_lower = response.text.lower()
        
        # æ£€æŸ¥å…³é”®è¯
        for keyword in blocked_keywords:
            if keyword in content_lower:
                return True
        
        # æ£€æŸ¥å“åº”å†…å®¹é•¿åº¦ï¼ˆå¯èƒ½è¿”å›ç©ºé¡µé¢æˆ–æå°é¡µé¢ï¼‰
        if len(response.text) < 500 and response.status_code == 200:
            # ä½†è¦æ’é™¤APIè¿”å›
            content_type = response.headers.get('Content-Type', '')
            if 'application/json' not in content_type and 'image/' not in content_type:
                return True
        
        return False
    
    def _handle_blocked(self) -> None:
        """å¤„ç†è¢«é˜»æ­¢çš„æƒ…å†µ"""
        # æ›´æ¢ä»£ç†
        self.protocol_obfuscator.rotate_proxy_chain()
        
        # åˆ·æ–°æŒ‡çº¹
        self._refresh_identity()
        
        # ä¼‘çœ ä¸€æ®µæ—¶é—´
        sleep_time = random.uniform(5, 15)
        print(f"[PhantomCrawler] ä¼‘çœ  {sleep_time:.2f} ç§’åé‡è¯•...")
        time.sleep(sleep_time)
    
    def _refresh_identity(self):
        """åˆ·æ–°çˆ¬è™«èº«ä»½ï¼ŒåŒ…æ‹¬æ›´æ¢æŒ‡çº¹å’Œä»£ç†"""
        # æ›´æ¢æŒ‡çº¹
        self.fingerprint_spoofer.refresh_fingerprint()
        
        # æ›´æ¢ä»£ç†
        self.protocol_obfuscator.rotate_proxy()
        
        # é‡æ–°åˆ›å»ºHTTPå®¢æˆ·ç«¯
        self.http_client = self._create_http_client()
        
        print(f"[PhantomCrawler] èº«ä»½å·²åˆ·æ–°")
    
    def _apply_strategies(self, strategies: Dict[str, Any]):
        """åº”ç”¨å…ƒè®¤çŸ¥ç³»ç»Ÿæ¨èçš„ç­–ç•¥"""
        # åº”ç”¨æŒ‡çº¹ç­–ç•¥
        fingerprint_strategy = strategies.get('fingerprint', {})
        if fingerprint_strategy.get('advanced', False):
            global_config.set('fingerprint.enable_advanced_spoofing', True)
        else:
            global_config.set('fingerprint.enable_advanced_spoofing', False)
        
        # åº”ç”¨å»¶è¿Ÿç­–ç•¥
        delay_value = strategies.get('delay', 2.0)
        global_config.set('behavior_simulation.min_delay', delay_value * 0.8)
        global_config.set('behavior_simulation.max_delay', delay_value * 1.2)
        
        # åº”ç”¨ä»£ç†ç­–ç•¥
        proxy_strategy = strategies.get('proxy')
        if proxy_strategy:
            if isinstance(proxy_strategy, list):
                global_config.set('proxy_chain', proxy_strategy)
            else:
                global_config.set('proxy_chain', [proxy_strategy] if proxy_strategy else [])
    
    def _generate_optimized_request_chain(self, url: str) -> List[str]:
        """ç”Ÿæˆç»è¿‡å…ƒè®¤çŸ¥ä¼˜åŒ–çš„è¯·æ±‚é“¾"""
        # è·å–ç­–ç•¥ä¸­çš„è¯·æ±‚é“¾
        base_resources = self.current_strategies.get('request_chain', [])
        
        # ç»“åˆé»˜è®¤çš„è¯·æ±‚é“¾ç”Ÿæˆç®—æ³•
        request_chain = []
        
        # æ·»åŠ åŸºç¡€èµ„æº
        request_chain.extend(base_resources)
        
        # ç¡®ä¿ç›®æ ‡URLåœ¨æœ€å
        if url not in request_chain:
            request_chain.append(url)
        elif request_chain[-1] != url:
            # å°†ç›®æ ‡URLç§»åˆ°æœ€å
            request_chain.remove(url)
            request_chain.append(url)
        
        return request_chain
    
    def _generate_desire_based_strategy(self, url: str):
        """åŸºäºä¸ƒå®—æ¬²ç”Ÿæˆå®æˆ˜ç­–ç•¥"""
        dominant = self.seven_desires.dominant_desire if hasattr(self.seven_desires, 'dominant_desire') else 'è´ªå©ª'
        risk = self.seven_desires.desire_perception.get('detection_danger', 0) if hasattr(self.seven_desires, 'desire_perception') else 0
        
        # æ ¹æ®ä¸»å¯¼æ¬²æœ›è°ƒæ•´ç­–ç•¥
        strategies = {
            'å‚²æ…¢': {
                'fingerprint': {'advanced': True, 'rotation_interval': 0},
                'delay': random.uniform(1.0, 2.0),
                'playwright_strategy': 'normal',
                'evasion_level': 1
            },
            'å«‰å¦’': {
                'fingerprint': {'advanced': True, 'rotation_interval': 5},
                'delay': random.uniform(2.0, 3.5),
                'playwright_strategy': 'stealth',
                'evasion_level': 2
            },
            'æ„¤æ€’': {
                'fingerprint': {'advanced': True, 'rotation_interval': 2},
                'delay': random.uniform(0.5, 1.5),
                'playwright_strategy': 'aggressive',
                'evasion_level': 3
            },
            'æ‡’æƒ°': {
                'fingerprint': {'advanced': False, 'rotation_interval': 10},
                'delay': random.uniform(3.0, 5.0),
                'playwright_strategy': 'stealth',
                'evasion_level': 1
            },
            'è´ªå©ª': {
                'fingerprint': {'advanced': True, 'rotation_interval': 3},
                'delay': random.uniform(1.5, 2.5),
                'playwright_strategy': 'normal',
                'evasion_level': 2
            },
            'æš´é£Ÿ': {
                'fingerprint': {'advanced': True, 'rotation_interval': 1},
                'delay': random.uniform(0.8, 1.8),
                'playwright_strategy': 'aggressive',
                'evasion_level': 2
            },
            'è‰²æ¬²': {
                'fingerprint': {'advanced': True, 'rotation_interval': 4},
                'delay': random.uniform(2.5, 4.0),
                'playwright_strategy': 'stealth',
                'evasion_level': 3
            }
        }
        
        # è·å–æ¬²æœ›å¯¹åº”çš„ç­–ç•¥
        base_strategy = strategies.get(dominant, strategies['è´ªå©ª'])
        
        # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´
        if risk > 0.7:
            base_strategy['evasion_level'] = 3
            base_strategy['fingerprint']['advanced'] = True
            base_strategy['delay'] = base_strategy['delay'] * 1.5
        elif risk > 0.4:
            base_strategy['evasion_level'] = max(1, base_strategy['evasion_level'])
            base_strategy['delay'] = base_strategy['delay'] * 1.2
        
        # æ›´æ–°å½“å‰ç­–ç•¥
        for k, v in base_strategy.items():
            self.current_strategies[k] = v
        
        # ç”Ÿæˆè¯·æ±‚é“¾
        if hasattr(self, '_generate_optimal_request_chain'):
            self.current_strategies['request_chain'] = self._generate_optimal_request_chain(url)
        self.current_strategies['risk_adjusted'] = True
        
        print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] å·²ç”Ÿæˆ{dominant}é©±åŠ¨ç­–ç•¥ - é£é™©ç­‰çº§: {risk:.2f}")
    
    def _get_desire_adjusted_delay(self):
        """æ ¹æ®ä¸ƒå®—æ¬²è·å–è°ƒæ•´åçš„å»¶è¿Ÿ"""
        dominant = self.seven_desires.dominant_desire if hasattr(self.seven_desires, 'dominant_desire') else 'è´ªå©ª'
        risk = self.seven_desires.desire_perception.get('detection_danger', 0) if hasattr(self.seven_desires, 'desire_perception') else 0
        
        # åŸºç¡€å»¶è¿Ÿ
        base_delay = self.current_strategies['delay']
        
        # æ¬²æœ›è°ƒæ•´ç³»æ•°
        desire_factors = {
            'å‚²æ…¢': 0.8,
            'å«‰å¦’': 1.2,
            'æ„¤æ€’': 0.5,
            'æ‡’æƒ°': 1.5,
            'è´ªå©ª': 1.0,
            'æš´é£Ÿ': 0.6,
            'è‰²æ¬²': 2.0
        }
        
        factor = desire_factors.get(dominant, 1.0)
        
        # é£é™©è°ƒæ•´
        if risk > 0.7:
            factor *= 1.8
        elif risk > 0.4:
            factor *= 1.3
        
        # éšæœºæ³¢åŠ¨
        jitter = random.uniform(0.8, 1.2)
        
        return max(0.2, base_delay * factor * jitter)
    
    def _get_risk_adjusted_headers(self):
        """è·å–åŸºäºé£é™©ç­‰çº§è°ƒæ•´çš„è¯·æ±‚å¤´"""
        risk = self.seven_desires.desire_perception.get('detection_danger', 0) if hasattr(self.seven_desires, 'desire_perception') else 0
        
        if risk > 0.5:
            # é«˜é£é™©æ—¶ç”Ÿæˆå…¨æ–°æŒ‡çº¹
            return self.fingerprint_spoofer.generate_fingerprint(advanced=True)
        return self.fingerprint_spoofer.generate_fingerprint()
    
    def _execute_main_request_with_desire(self, url: str):
        """æ‰§è¡Œä¸ƒå®—æ¬²ä¼˜åŒ–çš„ä¸»è¯·æ±‚"""
        # è¿™é‡Œå¤ç”¨ç°æœ‰çš„_execute_main_requestæ–¹æ³•
        # ä½†ä¼šæ ¹æ®ä¸ƒå®—æ¬²è¿›è¡Œå‚æ•°è°ƒæ•´
        return self._execute_main_request(url)
    
    def _seven_desires_analysis(self, url: str, result: Dict[str, Any], response_time: float, success: bool):
        """[å®æˆ˜ä¼˜åŒ–ç‰ˆ] æ‰§è¡Œä¸ƒå®—æ¬²åˆ†æï¼Œæ›´æ–°æ¬²æœ›å¼ºåº¦å’Œç­–ç•¥"""
        # å¢å¼ºé€’å½’é˜²æŠ¤æœºåˆ¶
        if not hasattr(self, '_analysis_recursion_depth'):
            self._analysis_recursion_depth = 0
        
        # åŒé‡é€’å½’é˜²æŠ¤ï¼šæ ‡å¿—æ£€æŸ¥ + æ·±åº¦é™åˆ¶
        if (hasattr(self, '_analysis_in_progress') and self._analysis_in_progress) or self._analysis_recursion_depth > 3:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] é€’å½’é˜²æŠ¤è§¦å‘: {'åˆ†æä¸­æ ‡å¿—' if self._analysis_in_progress else 'æ·±åº¦é™åˆ¶'}ï¼Œè·³è¿‡è°ƒç”¨")
            # å³ä½¿è¢«é˜²æŠ¤æ‹¦æˆªï¼Œä¹Ÿè¦ç¡®ä¿æ¸…ç†
            if self._analysis_recursion_depth > 3:
                self._analysis_recursion_depth -= 1
            return
        
        # æ›´æ–°é€’å½’æ·±åº¦å’ŒçŠ¶æ€æ ‡å¿—
        self._analysis_recursion_depth += 1
        self._analysis_in_progress = True
        
        try:
            # å¢å¼ºå®‰å…¨ç»“æœå¯¹è±¡æ„å»ºï¼Œæ·»åŠ æ›´å¤šå®æˆ˜ç›¸å…³å­—æ®µ
            safe_result = {
                'status_code': 500,
                'error': 'Unknown error',
                'blocked': False,
                'response_time': response_time,
                'timestamp': time.time(),
                'success': success
            }
            
            # å®‰å…¨åœ°æå–ç»“æœä¿¡æ¯ï¼Œé¿å…å¼‚å¸¸
            try:
                if isinstance(result, dict):
                    safe_result['status_code'] = result.get('status_code', 500)
                    safe_result['error'] = str(result.get('error', 'Unknown error'))
                    safe_result['blocked'] = result.get('blocked', False)
                    safe_result['content_length'] = len(result.get('content', '')) if 'content' in result else 0
                    safe_result['playwright_used'] = result.get('playwright_used', False)
                    safe_result['risk_level'] = result.get('risk_level', 0.5)
                else:
                    safe_result['error'] = str(result)
            except Exception as inner_e:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] æ„å»ºå®‰å…¨ç»“æœæ—¶å‡ºé”™: {str(inner_e)}")
            
            # å®æˆ˜ä¼˜åŒ–ï¼šæ·»åŠ è¯¦ç»†çš„æ—¥å¿—è®°å½•
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] åˆ†æURL: {url[:50]}{'...' if len(url) > 50 else ''}, çŠ¶æ€ç : {safe_result['status_code']}, æˆåŠŸ: {success}")
            
            # å®‰å…¨è®°å½•æˆåŠŸæˆ–å¤±è´¥
            try:
                if success:
                    if hasattr(self.seven_desires, 'record_success'):
                        try:
                            self.seven_desires.record_success(url, safe_result)
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] æˆåŠŸè®°å½•: {url}")
                            # å®æˆ˜ä¼˜åŒ–ï¼šæˆåŠŸæ—¶å°å¹…é™ä½é£é™©è¯„ä¼°
                            if hasattr(self.seven_desires, 'update_risk_level') and safe_result['risk_level'] > 0.1:
                                self.seven_desires.update_risk_level(url, -0.05)
                        except Exception as e:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] è®°å½•æˆåŠŸå¤±è´¥: {str(e)}")
                else:
                    if hasattr(self.seven_desires, 'record_failure'):
                        try:
                            self.seven_desires.record_failure(url, safe_result)
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] å¤±è´¥è®°å½•: {url}")
                            # å®æˆ˜ä¼˜åŒ–ï¼šå¤±è´¥æ—¶æ™ºèƒ½å¢åŠ é£é™©è¯„ä¼°
                            risk_increase = 0.15 if safe_result['blocked'] else 0.08
                            if hasattr(self.seven_desires, 'update_risk_level'):
                                self.seven_desires.update_risk_level(url, risk_increase)
                        except Exception as e:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] è®°å½•å¤±è´¥å¤±è´¥: {str(e)}")
            except Exception as record_e:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] è®°å½•æ“ä½œå¼‚å¸¸: {str(record_e)}")
            
            # å®æˆ˜ä¼˜åŒ–ï¼šæ ¹æ®é€’å½’æ·±åº¦å’Œå®‰å…¨çŠ¶æ€ï¼Œé€‰æ‹©æ€§åœ°å¯ç”¨é«˜çº§åŠŸèƒ½
            try:
                if self._analysis_recursion_depth == 1:  # ä»…åœ¨é¡¶å±‚è°ƒç”¨æ—¶æ‰§è¡Œ
                    # æ™ºèƒ½æ¢å¤éƒ¨åˆ†é«˜çº§åŠŸèƒ½ï¼Œä½†å¢åŠ å®‰å…¨æ£€æŸ¥
                    if hasattr(self.seven_desires, '_sense_danger') and not safe_result['success']:
                        try:
                            self.seven_desires._sense_danger(safe_result['success'], safe_result)
                        except Exception as sense_e:
                            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] å±é™©æ„ŸçŸ¥å¼‚å¸¸: {str(sense_e)}")
            except Exception as advanced_e:
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] é«˜çº§åŠŸèƒ½å¼‚å¸¸: {str(advanced_e)}")
                
        except Exception as e:
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] åˆ†æä¸»å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            # è®°å½•å¼‚å¸¸åˆ°å¤±è´¥å†å²
            if hasattr(self, 'error_history'):
                try:
                    self.error_history.append({
                        'method': '_seven_desires_analysis',
                        'error': str(e),
                        'url': url,
                        'timestamp': time.time()
                    })
                except:
                    pass
        finally:
            # ç¡®ä¿æ¸…ç†é€’å½’çŠ¶æ€ï¼Œé˜²æ­¢æ°¸ä¹…é”å®š
            try:
                self._analysis_recursion_depth = max(0, self._analysis_recursion_depth - 1)
                self._analysis_in_progress = False
            except:
                # ç»ˆæå®‰å…¨ä¿éšœï¼Œç¡®ä¿çŠ¶æ€é‡ç½®
                setattr(self, '_analysis_recursion_depth', 0)
                setattr(self, '_analysis_in_progress', False)
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«-å®æˆ˜ç‰ˆ] åˆ†æå®Œæˆï¼Œé€’å½’æ·±åº¦é‡ç½®: {self._analysis_recursion_depth}")
    
    def _metacognitive_analysis(self, url: str, result: Dict[str, Any], response_time: float):
        """ä¿æŒå‘åå…¼å®¹çš„å…ƒè®¤çŸ¥åˆ†ææ–¹æ³•"""
        # è°ƒç”¨ä¸ƒå®—æ¬²åˆ†æ
        success = result.get('status_code', 0) < 400 and not self._is_blocked_content(result.get('content', ''))
        self._seven_desires_analysis(url, result, response_time, success)
        
        # æ›´æ–°æˆåŠŸè¿ç»­æ¬¡æ•°
        if result['status_code'] == 200 and not self._is_blocked_content(result.get('content', '')):
            self.success_streak += 1
        else:
            self.success_streak = 0
        
        # è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡
        performance_metrics = self.self_awareness.get_performance_metrics()
        
        # ç¼–ç å½“å‰çŠ¶æ€
        current_state = self.learning_optimizer.encode_state({
            'success_rate': performance_metrics['success_rate'],
            'avg_response_time': performance_metrics['avg_response_time'],
            'error_rate': sum(self.self_awareness.error_counts.values()) / max(1, len(self.crawl_history)),
            'resource_pressure': self.self_awareness.get_resource_metrics()['cpu_usage']['average'] / 100
        })
        
        # è®¡ç®—å¥–åŠ±
        reward = self.learning_optimizer.calculate_reward(
            result['status_code'] == 200 and not self._is_blocked_content(result.get('content', '')),
            {
                'response_time': response_time,
                'resource_usage': self.self_awareness.get_resource_metrics()['cpu_usage']['average'] / 100,
                'success_streak': self.success_streak
            }
        )
        
        # æ›´æ–°å­¦ä¹ 
        if hasattr(self, 'previous_state') and hasattr(self, 'previous_action'):
            self.learning_optimizer.learn(self.previous_state, self.previous_action, reward, current_state)
            self.learning_optimizer.store_experience(self.previous_state, self.previous_action, reward, current_state)
        
        # é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œ
        action = self.learning_optimizer.select_action(current_state)
        
        # æ›´æ–°çŠ¶æ€
        self.previous_state = current_state
        self.previous_action = action
        
        # å®šæœŸä»ç»éªŒä¸­å­¦ä¹ 
        if len(self.crawl_history) % 10 == 0:
            self.learning_optimizer.replay_experiences()
    
    def _record_failure(self, url: str, error_message: str):
        """[å®æˆ˜ä¼˜åŒ–ç‰ˆ] è®°å½•å¤±è´¥å¹¶æ‰§è¡Œæ™ºèƒ½å­¦ä¹ ä¸è‡ªé€‚åº”è°ƒæ•´"""
        # å®æˆ˜ä¼˜åŒ–ï¼šæ·»åŠ è®°å½•å¤±è´¥çš„å¼€å§‹æ—¥å¿—
        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å¼€å§‹è®°å½•å¤±è´¥: {url[:50]}{'...' if len(url) > 50 else ''}")
        
        # ç¡®ä¿success_streakå±æ€§å­˜åœ¨å¹¶é‡ç½®
        if not hasattr(self, 'success_streak'):
            self.success_streak = 0
        self.success_streak = 0
        
        # å®æˆ˜ä¼˜åŒ–ï¼šè®°å½•å¤±è´¥æ¬¡æ•°å’Œå¤±è´¥ç±»å‹ç»Ÿè®¡
        if not hasattr(self, 'failure_stats'):
            self.failure_stats = {'total': 0, 'types': {}}
        self.failure_stats['total'] += 1
        
        # åˆ†æé”™è¯¯ç±»å‹
        error_type = 'unknown'
        error_lower = str(error_message).lower() if error_message else ''
        
        # å®æˆ˜ä¼˜åŒ–ï¼šæ›´ç»†ç²’åº¦çš„é”™è¯¯ç±»å‹è¯†åˆ«
        error_patterns = {
            'timeout': ['timeout', 'timed out', 'connection timed out'],
            'connection': ['connection', 'connect', 'refused', 'reset', 'closed'],
            'block': ['block', 'captcha', 'éªŒè¯ç ', 'robot', 'automated', 'suspicious'],
            'server': ['500', '502', '503', '504', 'server error'],
            'network': ['network', 'dns', 'resolve', 'unreachable'],
            'playwright': ['browser', 'page', 'context', 'playwright']
        }
        
        for pattern_type, patterns in error_patterns.items():
            if any(p in error_lower for p in patterns):
                error_type = pattern_type
                break
        
        # æ›´æ–°å¤±è´¥ç»Ÿè®¡
        self.failure_stats['types'][error_type] = self.failure_stats['types'].get(error_type, 0) + 1
        
        # æ„å»ºå¢å¼ºçš„å®‰å…¨ç»“æœå¯¹è±¡
        safe_result = {
            'status_code': 500,
            'error': str(error_message),
            'error_type': error_type,
            'url': url,
            'timestamp': time.time(),
            'retry_count': getattr(self, 'current_retry_count', 0)
        }
        
        try:
            # 1. è®°å½•å¤±è´¥åˆ°ä¸ƒå®—æ¬²ç³»ç»Ÿ
            if hasattr(self.seven_desires, 'record_failure'):
                try:
                    self.seven_desires.record_failure(url, safe_result)
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å¤±è´¥è®°å½•å·²ä¿å­˜: {url}, é”™è¯¯ç±»å‹: {error_type}")
                except Exception as record_e:
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] è®°å½•å¤±è´¥åˆ°ä¸ƒå®—æ¬²ç³»ç»Ÿå‡ºé”™: {str(record_e)}")
            
            # 2. å®æˆ˜ä¼˜åŒ–ï¼šåŸºäºé”™è¯¯ç±»å‹çš„å³æ—¶å“åº”
            try:
                # è¿æ¥é”™è¯¯ç«‹å³æ›´æ¢ä»£ç†
                if error_type == 'connection' and hasattr(self, 'protocol_obfuscator'):
                    self.protocol_obfuscator.rotate_proxy()
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ£€æµ‹åˆ°è¿æ¥é”™è¯¯ï¼Œå·²ç«‹å³æ›´æ¢ä»£ç†")
                
                # é˜»æ­¢é”™è¯¯ç«‹å³åˆ·æ–°èº«ä»½
                elif error_type == 'block' and hasattr(self, '_refresh_identity'):
                    self._refresh_identity()
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ£€æµ‹åˆ°é˜»æ­¢é”™è¯¯ï¼Œå·²ç«‹å³åˆ·æ–°èº«ä»½")
                
                # è¶…æ—¶é”™è¯¯å¢åŠ è¶…æ—¶è®¾ç½®
                elif error_type == 'timeout':
                    if hasattr(self, 'global_config'):
                        current_timeout = self.global_config.get('request_timeout', 30)
                        new_timeout = min(120, current_timeout * 1.5)
                        self.global_config.set('request_timeout', new_timeout)
                        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ£€æµ‹åˆ°è¶…æ—¶ï¼Œè¶…æ—¶æ—¶é—´å·²è°ƒæ•´è‡³: {new_timeout}ç§’")
            except Exception as immediate_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å³æ—¶å“åº”å¤„ç†å‡ºé”™: {str(immediate_e)}")
            
            # 3. æ™ºèƒ½æ¨¡å¼æ£€æµ‹ä¸è‡ªé€‚åº” - å¢åŠ æ›´å¤šå®‰å…¨æ£€æŸ¥
            try:
                # ç¡®ä¿crawl_historyå­˜åœ¨ä¸”æœ‰è¶³å¤Ÿæ•°æ®
                if hasattr(self, 'crawl_history') and len(self.crawl_history) >= 3:
                    # ä½¿ç”¨æ›´å¤šå†å²æ•°æ®è¿›è¡Œæ›´å‡†ç¡®çš„æ¨¡å¼æ£€æµ‹
                    history_size = min(10, len(self.crawl_history))
                    recent_results = self.crawl_history[-history_size:]
                    
                    # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„å†å²è®°å½•
                    valid_results = []
                    for h in recent_results:
                        if isinstance(h, dict) and 'url' in h and 'timestamp' in h:
                            valid_results.append(h)
                    
                    # åªæœ‰å½“æœ‰è¶³å¤Ÿæœ‰æ•ˆæ•°æ®æ—¶æ‰è¿›è¡Œæ¨¡å¼æ£€æµ‹
                    if len(valid_results) >= 3 and hasattr(self.seven_desires, 'detect_pattern_changes'):
                        try:
                            # å¢åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                            context_info = {
                                'recent_error_rate': sum(1 for r in valid_results if r.get('blocked', False)) / len(valid_results),
                                'error_type': error_type,
                                'total_failures': self.failure_stats['total']
                            }
                            
                            # å®‰å…¨è°ƒç”¨æ¨¡å¼æ£€æµ‹
                            if self.seven_desires.detect_pattern_changes(url, valid_results, context_info):
                                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ£€æµ‹åˆ°æ¨¡å¼å˜åŒ–ï¼Œå‡†å¤‡ç”Ÿæˆè‡ªé€‚åº”å“åº”")
                                
                                # ç”Ÿæˆå¹¶åº”ç”¨è‡ªé€‚åº”å“åº”
                                if hasattr(self.seven_desires, 'generate_adaptive_response'):
                                    adaptive_response = self.seven_desires.generate_adaptive_response(
                                        url, 
                                        True,
                                        error_type=error_type,
                                        retry_count=getattr(self, 'current_retry_count', 0)
                                    )
                                    
                                    if adaptive_response and isinstance(adaptive_response, dict):
                                        # å®æˆ˜ä¼˜åŒ–ï¼šå¢å¼ºè‡ªé€‚åº”å“åº”çš„åº”ç”¨
                                        enhanced_response = {
                                            **adaptive_response,
                                            'error_type': error_type,
                                            'applied_timestamp': time.time()
                                        }
                                        self._apply_adaptive_response(enhanced_response)
                        except Exception as pattern_e:
                            print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ¨¡å¼æ£€æµ‹å’Œè‡ªé€‚åº”ç”Ÿæˆå‡ºé”™: {str(pattern_e)}")
            except Exception as pattern_overall_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ¨¡å¼åˆ†ææ€»ä½“å¼‚å¸¸: {str(pattern_overall_e)}")
            
            # 4. å®æˆ˜ä¼˜åŒ–ï¼šé£é™©çº§åˆ«åŠ¨æ€è°ƒæ•´
            try:
                if hasattr(self.seven_desires, 'update_risk_level'):
                    # åŸºäºé”™è¯¯ç±»å‹è°ƒæ•´é£é™©å¢åŠ å¹…åº¦
                    risk_increments = {
                        'block': 0.3,       # æœ€é«˜é£é™©
                        'timeout': 0.15,
                        'connection': 0.1,
                        'playwright': 0.2,
                        'server': 0.05,
                        'network': 0.08,
                        'unknown': 0.1
                    }
                    
                    increment = risk_increments.get(error_type, 0.1)
                    # å¤±è´¥æ¬¡æ•°è¶Šå¤šï¼Œé£é™©å¢åŠ è¶Šå°‘ï¼ˆé¿å…é£é™©å€¼è¿‡å¿«è¾¾åˆ°ä¸Šé™ï¼‰
                    if self.failure_stats['total'] > 10:
                        increment *= 0.7
                    
                    self.seven_desires.update_risk_level(url, increment)
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] é£é™©çº§åˆ«æ›´æ–°: +{increment}, é”™è¯¯ç±»å‹: {error_type}")
            except Exception as risk_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] é£é™©çº§åˆ«æ›´æ–°å‡ºé”™: {str(risk_e)}")
                
        except Exception as e:
            # ç»ˆæå¼‚å¸¸æ•è·ï¼Œç¡®ä¿ä¸ä¼šä¸­æ–­
            print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] è®°å½•å¤±è´¥ä¸»å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            
            # å³ä½¿å‡ºé”™ä¹Ÿè¦ä¿å­˜é”™è¯¯è®°å½•
            if hasattr(self, 'error_history'):
                try:
                    self.error_history.append({
                        'method': '_record_failure',
                        'error': str(e),
                        'url': url,
                        'timestamp': time.time()
                    })
                except:
                    pass
        finally:
            # å®æˆ˜ä¼˜åŒ–ï¼šæ·»åŠ å®Œæˆæ—¥å¿—
            print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å¤±è´¥è®°å½•å¤„ç†å®Œæˆ: {url[:50]}{'...' if len(url) > 50 else ''}, é”™è¯¯ç±»å‹: {error_type}")
    
    def _is_blocked_content(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦è¢«é˜»æ­¢"""
        blocked_keywords = [
            'captcha', 'éªŒè¯ç ', 'robot', 'automated', 'blocked', 
            'suspicious', 'unusual activity', 'access denied'
        ]
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in blocked_keywords)
    
    def _adjust_strategy_based_on_error(self, error_type: str):
        """æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­–ç•¥"""
        if error_type == 'timeout':
            # è¶…æ—¶é”™è¯¯ï¼Œå¢åŠ å»¶è¿Ÿå’Œè¶…æ—¶æ—¶é—´
            current_timeout = global_config.get('request_timeout', 30)
            global_config.set('request_timeout', min(60, current_timeout + 5))
            print(f"[ä¸ƒå®—æ¬²å¼•æ“] æ£€æµ‹åˆ°è¶…æ—¶ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´è‡³ {global_config.get('request_timeout')}")
        elif error_type == 'connection_error':
            # è¿æ¥é”™è¯¯ï¼Œæ›´æ¢ä»£ç†
            self.protocol_obfuscator.rotate_proxy()
            print(f"[ä¸ƒå®—æ¬²å¼•æ“] æ£€æµ‹åˆ°è¿æ¥é”™è¯¯ï¼Œå·²æ›´æ¢ä»£ç†")
    
    def _metacognitive_adaptation(self, url: str, detection_info: Dict[str, Any]):
        """[å®æˆ˜ä¼˜åŒ–ç‰ˆ] æ‰§è¡Œå…ƒè®¤çŸ¥è‡ªé€‚åº”è°ƒæ•´ï¼Œå¢å¼ºå®æˆ˜çªç ´èƒ½åŠ›"""
        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å¼€å§‹å…ƒè®¤çŸ¥è‡ªé€‚åº”è°ƒæ•´: {url[:50]}{'...' if len(url) > 50 else ''}")
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„å±æ€§å­˜åœ¨
        if not hasattr(self, 'crawl_history'):
            self.crawl_history = []
        if not hasattr(self, 'adaptation_history'):
            self.adaptation_history = []
        
        try:
            # 1. å¢å¼ºçš„æ¨¡å¼æ£€æµ‹ - è€ƒè™‘æ›´å¤šå†å²æ•°æ®å’Œä¸Šä¸‹æ–‡
            try:
                # ä½¿ç”¨æ›´å¤§çš„å†å²çª—å£è¿›è¡Œæ›´å‡†ç¡®çš„æ¨¡å¼æ£€æµ‹
                history_size = min(15, len(self.crawl_history))
                recent_results = self.crawl_history[-history_size:] if len(self.crawl_history) >= history_size else self.crawl_history
                
                # è¿‡æ»¤å¹¶éªŒè¯å†å²æ•°æ®
                valid_history = []
                for h in recent_results:
                    if isinstance(h, dict) and 'url' in h and 'status_code' in h:
                        # æ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°æ¯ä¸ªå†å²è®°å½•
                        enhanced_record = {
                            **h,
                            'is_blocked': h.get('blocked', False),
                            'response_time': h.get('response_time', 0),
                            'risk_level': h.get('risk_level', 0.5)
                        }
                        valid_history.append(enhanced_record)
                
                # åªæœ‰å½“æœ‰è¶³å¤Ÿæœ‰æ•ˆæ•°æ®æ—¶æ‰è¿›è¡Œæ¨¡å¼æ£€æµ‹
                if len(valid_history) >= 3 and hasattr(self.seven_desires, 'detect_pattern_changes'):
                    # æ„å»ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                    context_info = {
                        'recent_block_rate': sum(1 for h in valid_history if h['is_blocked']) / len(valid_history),
                        'avg_response_time': sum(h.get('response_time', 0) for h in valid_history) / len(valid_history),
                        'detection_info': detection_info,
                        'current_time': time.time(),
                        'total_adaptations': len(self.adaptation_history)
                    }
                    
                    pattern_changed = self.seven_desires.detect_pattern_changes(url, valid_history, context_info)
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ¨¡å¼æ£€æµ‹ç»“æœ: {'å˜åŒ–' if pattern_changed else 'ç¨³å®š'}")
                else:
                    pattern_changed = False
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å†å²æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ¨¡å¼æ£€æµ‹")
            except Exception as pattern_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æ¨¡å¼æ£€æµ‹å¼‚å¸¸: {str(pattern_e)}")
                pattern_changed = False  # å‡ºé”™æ—¶é»˜è®¤å‡è®¾æ¨¡å¼ç¨³å®š
            
            # 2. æ™ºèƒ½è‡ªé€‚åº”å“åº”ç”Ÿæˆ
            try:
                blocked_status = detection_info.get('blocked', False)
                captcha_detected = detection_info.get('captcha_detected', False)
                
                # æ ¹æ®ä¸åŒçš„æ£€æµ‹æƒ…å†µç”Ÿæˆä¸åŒå¼ºåº¦çš„å“åº”
                response_intensity = 'mild'
                if captcha_detected:
                    response_intensity = 'maximum'  # éªŒè¯ç éœ€è¦æœ€å¼ºå“åº”
                elif blocked_status:
                    response_intensity = 'strong'    # é˜»æ­¢éœ€è¦å¼ºå“åº”
                elif pattern_changed:
                    response_intensity = 'moderate'  # æ¨¡å¼å˜åŒ–éœ€è¦ä¸­ç­‰å“åº”
                
                # æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°å“åº”ç”Ÿæˆ
                response_context = {
                    'intensity': response_intensity,
                    'error_type': detection_info.get('error_type', 'unknown'),
                    'status_code': detection_info.get('status_code', 0),
                    'risk_level': detection_info.get('risk_level', 0.5)
                }
                
                if hasattr(self.seven_desires, 'generate_adaptive_response'):
                    adaptive_response = self.seven_desires.generate_adaptive_response(
                        url,
                        pattern_changed or blocked_status,
                        context=response_context
                    )
                    
                    # åº”ç”¨è‡ªé€‚åº”å“åº”
                    if adaptive_response and isinstance(adaptive_response, dict):
                        self._apply_adaptive_response(adaptive_response)
                        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å·²åº”ç”¨è‡ªé€‚åº”å“åº”ï¼Œå¼ºåº¦: {response_intensity}")
                    else:
                        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] æœªç”Ÿæˆæœ‰æ•ˆçš„è‡ªé€‚åº”å“åº”")
            except Exception as response_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] è‡ªé€‚åº”å“åº”ç”Ÿæˆå¼‚å¸¸: {str(response_e)}")
            
            # 3. å¢å¼ºçš„å­¦ä¹ ä¼˜åŒ–å»ºè®® - æ·»åŠ æ›´å¤šå®‰å…¨æ£€æŸ¥
            try:
                # å®‰å…¨è·å–æ€§èƒ½æŒ‡æ ‡
                performance_metrics = {}
                if hasattr(self, 'self_awareness') and hasattr(self.self_awareness, 'get_performance_metrics'):
                    try:
                        performance_metrics = self.self_awareness.get_performance_metrics()
                    except Exception as perf_e:
                        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] è·å–æ€§èƒ½æŒ‡æ ‡å¼‚å¸¸: {str(perf_e)}")
                        # ä½¿ç”¨é»˜è®¤å€¼
                        performance_metrics = {
                            'success_rate': 0.5,
                            'avg_response_time': 3.0,
                            'current_pattern': 'balanced'
                        }
                
                # å®‰å…¨è®¡ç®—é”™è¯¯ç‡
                error_rate = 0.0
                try:
                    total_errors = 0
                    if hasattr(self, 'self_awareness') and hasattr(self.self_awareness, 'error_counts'):
                        total_errors = sum(self.self_awareness.error_counts.values())
                    error_rate = total_errors / max(1, len(self.crawl_history))
                except:
                    error_rate = 0.5  # é»˜è®¤å€¼
                
                # æ„å»ºä¼˜åŒ–å»ºè®®è¯·æ±‚
                adaptation_input = {
                    'success_rate': performance_metrics.get('success_rate', 0.5),
                    'avg_response_time': performance_metrics.get('avg_response_time', 3.0),
                    'error_rate': error_rate,
                    'current_pattern': performance_metrics.get('current_pattern', 'balanced'),
                    'blocked': detection_info.get('blocked', False),
                    'captcha_detected': detection_info.get('captcha_detected', False)
                }
                
                # å®‰å…¨è¯·æ±‚ä¼˜åŒ–å»ºè®®
                adaptation_suggestion = None
                if hasattr(self, 'learning_optimizer') and hasattr(self.learning_optimizer, 'suggest_adaptation'):
                    try:
                        adaptation_suggestion = self.learning_optimizer.suggest_adaptation(adaptation_input)
                    except Exception as suggest_e:
                        print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] è·å–ä¼˜åŒ–å»ºè®®å¼‚å¸¸: {str(suggest_e)}")
                
                # åº”ç”¨ä¼˜åŒ–å»ºè®®
                if adaptation_suggestion and isinstance(adaptation_suggestion, dict):
                    self._apply_optimization_suggestions(adaptation_suggestion)
                    print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å·²åº”ç”¨å­¦ä¹ ä¼˜åŒ–å»ºè®®")
            except Exception as learning_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å­¦ä¹ ä¼˜åŒ–å¼‚å¸¸: {str(learning_e)}")
            
            # 4. å®æˆ˜ä¼˜åŒ–ï¼šè®°å½•è‡ªé€‚åº”å†å²ï¼Œç”¨äºåç»­åˆ†æ
            try:
                adaptation_record = {
                    'url': url,
                    'timestamp': time.time(),
                    'blocked': detection_info.get('blocked', False),
                    'captcha_detected': detection_info.get('captcha_detected', False),
                    'pattern_changed': pattern_changed,
                    'risk_level': detection_info.get('risk_level', 0.5)
                }
                self.adaptation_history.append(adaptation_record)
                
                # é™åˆ¶å†å²è®°å½•å¤§å°
                if len(self.adaptation_history) > 100:
                    self.adaptation_history = self.adaptation_history[-100:]
            except Exception as history_e:
                print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] è®°å½•è‡ªé€‚åº”å†å²å¼‚å¸¸: {str(history_e)}")
                
        except Exception as e:
            # ç»ˆæå¼‚å¸¸æ•è·
            print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å…ƒè®¤çŸ¥è‡ªé€‚åº”ä¸»å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            
            # å³ä½¿å‡ºé”™ä¹Ÿè¦è®°å½•
            if hasattr(self, 'error_history'):
                try:
                    self.error_history.append({
                        'method': '_metacognitive_adaptation',
                        'error': str(e),
                        'url': url,
                        'timestamp': time.time()
                    })
                except:
                    pass
        finally:
            print(f"[PhantomCrawler-å®æˆ˜ç‰ˆ] å…ƒè®¤çŸ¥è‡ªé€‚åº”å®Œæˆ: {url[:50]}{'...' if len(url) > 50 else ''}")
    
    def _apply_adaptive_response(self, response: Dict[str, Any]):
        """åº”ç”¨è‡ªé€‚åº”å“åº”ç­–ç•¥"""
        if response.get('fingerprint_reset', False):
            self.fingerprint_spoofer.reset_fingerprint()
            print(f"[ä¸ƒå®—æ¬²å¼•æ“] é‡ç½®æŒ‡çº¹")
        
        if response.get('delay_increase_factor', 1.0) > 1.0:
            current_min = global_config.get('behavior_simulation.min_delay', 1.0)
            current_max = global_config.get('behavior_simulation.max_delay', 3.0)
            factor = response['delay_increase_factor']
            global_config.set('behavior_simulation.min_delay', current_min * factor)
            global_config.set('behavior_simulation.max_delay', current_max * factor)
            print(f"[ä¸ƒå®—æ¬²å¼•æ“] å¢åŠ å»¶è¿Ÿå› å­: {factor}")
        
        if response.get('force_proxy_change', False):
            self.protocol_obfuscator.force_proxy_change()
            print(f"[ä¸ƒå®—æ¬²å¼•æ“] å¼ºåˆ¶æ›´æ¢ä»£ç†")
        
        if response.get('behavior_shift', False):
            self.behavior_simulator.shift_behavior_pattern()
            print(f"[ä¸ƒå®—æ¬²å¼•æ“] åˆ‡æ¢æ¬²æœ›æ¨¡å¼")
    
    def _apply_optimization_suggestions(self, suggestions: Dict[str, Any]):
        """åº”ç”¨å­¦ä¹ ä¼˜åŒ–å™¨çš„å»ºè®®"""
        adaptations = suggestions.get('suggested_adaptations', [])
        
        for adaptation in adaptations:
            if adaptation['priority'] in ['critical', 'high']:
                actions = adaptation['actions']
                
                for action in actions:
                    if action == 'change_fingerprint':
                        self.fingerprint_spoofer.refresh_fingerprint()
                    elif action == 'switch_proxy':
                        self.protocol_obfuscator.rotate_proxy()
                    elif action == 'increase_delay':
                        current_min = global_config.get('behavior_simulation.min_delay', 1.0)
                        current_max = global_config.get('behavior_simulation.max_delay', 3.0)
                        global_config.set('behavior_simulation.min_delay', current_min * 1.5)
                        global_config.set('behavior_simulation.max_delay', current_max * 1.5)
                    elif action == 'enhance_fingerprint':
                        global_config.set('fingerprint.enable_advanced_spoofing', True)
                    elif action == 'reset_all':
                        self._refresh_identity()
                        self.success_streak = 0
        
        # æ›´æ–°ç­–ç•¥æ€§èƒ½ç»Ÿè®¡
        if adaptations:
            strategy_type = adaptations[0].get('type', 'general')
            self.learning_optimizer.update_strategy_performance(
                strategy_type,
                suggestions.get('confidence', 0.5) > 0.7,  # åŸºäºç½®ä¿¡åº¦åˆ¤æ–­æˆåŠŸ
                suggestions.get('confidence', 0.5) * 10  # å¥–åŠ±ä¸ç½®ä¿¡åº¦ç›¸å…³
            )
    
    def get_metacognitive_insights(self) -> Dict[str, Any]:
        """
        è·å–å…ƒè®¤çŸ¥ç³»ç»Ÿçš„æ´å¯Ÿå’Œç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«å…ƒè®¤çŸ¥æ´å¯Ÿçš„å­—å…¸
        """
        return {
            'self_awareness': {
                'performance': self.self_awareness.get_performance_metrics(),
                'resources': self.self_awareness.get_resource_metrics(),
                'environment': self.self_awareness.get_environment_assessment()
            },
            'learning': self.learning_optimizer.get_learning_summary(),
            'current_strategies': self.current_strategies,
            'success_streak': self.success_streak
        }
    
    def close(self):
        """å…³é—­çˆ¬è™«å¹¶é‡Šæ”¾æ‰€æœ‰èµ„æº"""
        # å…³é—­è‡ªæˆ‘æ„ŸçŸ¥ç›‘æ§å™¨
        if hasattr(self, 'self_awareness'):
            self.self_awareness.shutdown()
        
        # ä¿å­˜å…ƒè®¤çŸ¥çŸ¥è¯†
        if hasattr(self, 'seven_desires'):
            self.seven_desires._save_desire_knowledge()
        
        # å…³é—­HTTPå®¢æˆ·ç«¯
        if self.http_client:
            self.http_client.close()
        
        # å…³é—­Playwrightæµè§ˆå™¨
        if self.playwright_browser:
            self.playwright_browser.close()
        
        self.is_running = False
        print(f"[PhantomCrawler] å·²å…³é—­ï¼Œä¼šè¯ID: {self.session_id}")
    
    def _crawl_with_playwright(self, url: str, callback: Optional[Callable] = None, force_strategy: Optional[str] = None) -> Dict[str, Any]:
        """ä½¿ç”¨Playwrightè¿›è¡Œæ™ºèƒ½çˆ¬å–ï¼Œé›†æˆä¸ƒå®—æ¬²å¼•æ“çš„ç¯å¢ƒæ„ŸçŸ¥å’Œé£é™©è¯„ä¼°ï¼ˆå®æˆ˜ç‰ˆï¼‰"""
        try:
            from playwright.sync_api import sync_playwright
            
            # ä»ä¸ƒå®—æ¬²å¼•æ“è·å–é£é™©è¯„ä¼°
            risk_level = self.seven_desires.desire_perception['detection_danger'] if hasattr(self.seven_desires, 'desire_perception') else 0
            dominant_desire = self.seven_desires.dominant_desire if hasattr(self.seven_desires, 'dominant_desire') else 'è´ªå©ª'
            
            print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] Playwrightæ¨¡å¼ - {dominant_desire}é©±åŠ¨ - é£é™©ç­‰çº§: {risk_level:.2f}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            with sync_playwright() as p:
                # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´æµè§ˆå™¨å¯åŠ¨å‚æ•°
                browser_args = [
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                ]
                
                # é«˜é£é™©æ—¶æ·»åŠ æ›´å¤šçš„åæ£€æµ‹å‚æ•°
                if risk_level > 0.7:
                    browser_args.extend([
                        '--disable-gpu',
                        '--window-size=1920,1080',
                        '--disable-features=site-per-process',
                        '--disable-blink-features=ScriptStreaming',
                        '--start-maximized',
                        '--disable-infobars',
                        '--ignore-certificate-errors',
                        '--disable-software-rasterizer',
                        '--no-first-run',
                        '--no-default-browser-check',
                        '--disable-extensions',
                    ])
                
                # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´slow_moå‚æ•°
                slow_mo_value = random.randint(50, 150)
                if risk_level > 0.5:
                    slow_mo_value = random.randint(100, 200)  # æ›´é«˜é£é™©æ—¶æ›´æ…¢
                elif risk_level < 0.3:
                    slow_mo_value = random.randint(30, 80)  # ä½é£é™©æ—¶å¯ä»¥æ›´å¿«
                
                # å¯åŠ¨æµè§ˆå™¨
                browser = p.chromium.launch(
                    headless=global_config.get('playwright.headless', risk_level < 0.6),  # é«˜é£é™©æ—¶ä½¿ç”¨æœ‰å¤´æ¨¡å¼
                    args=browser_args,
                    slow_mo=slow_mo_value
                )
                
                # è·å–å½“å‰è¡Œä¸ºæ¨¡å¼
                pattern = self.behavior_simulator.behavior_pattern
                
                # é«˜é£é™©æ¨¡å¼æˆ–stealthæ¨¡å¼ä¸‹ä½¿ç”¨é«˜çº§æŒ‡çº¹
                if risk_level > 0.5 or pattern == 'stealth':
                    fingerprint_headers = self.fingerprint_spoofer.generate_advanced_fingerprint()
                else:
                    fingerprint_headers = self.fingerprint_spoofer.generate_fingerprint()
                
                # æ ¹æ®è¡Œä¸ºæ¨¡å¼é€‰æ‹©å±å¹•å°ºå¯¸åˆ†å¸ƒ
                if pattern == 'careful' or pattern == 'stealth':
                    # æ›´å¸¸è§çš„æ¡Œé¢åˆ†è¾¨ç‡
                    width = random.choice([1366, 1440, 1920])
                    height = random.choice([768, 900, 1080])
                elif pattern == 'hurried':
                    # å¯èƒ½ä½¿ç”¨ç¬”è®°æœ¬æˆ–ç§»åŠ¨è®¾å¤‡
                    width = random.choice([1366, 1440, 1024, 375, 414])
                    height = random.choice([768, 900, 768, 667, 896])
                else:
                    # é»˜è®¤æ›´å¹¿æ³›çš„é€‰æ‹©
                    width = random.choice([1366, 1440, 1536, 1920, 2560])
                    height = random.choice([768, 900, 1080, 1440])
                
                # åˆ›å»ºä¸Šä¸‹æ–‡å¹¶é…ç½®æŒ‡çº¹
                context = browser.new_context(
                    user_agent=fingerprint_headers['User-Agent'],
                    locale=random.choice(['en-US', 'zh-CN', 'ja-JP']),
                    timezone_id=random.choice(['America/New_York', 'Europe/London', 'Asia/Shanghai']),
                    screen={
                        'width': width,
                        'height': height
                    },
                    viewport={'width': width, 'height': height},
                    user_data_dir=None  # é¿å…ä½¿ç”¨æŒä¹…åŒ–ç”¨æˆ·æ•°æ®
                )
                
                # é…ç½®é¢å¤–çš„æŒ‡çº¹æ··æ·†
                context.set_extra_http_headers({
                    'Accept-Language': fingerprint_headers.get('Accept-Language', 'en-US,en;q=0.9'),
                    'Accept-Encoding': fingerprint_headers.get('Accept-Encoding', 'gzip, deflate, br'),
                })
                
                # ç¦ç”¨è‡ªåŠ¨åŒ–ç‰¹å¾
                page = context.new_page()
                
                # æ ¹æ®ä¸ƒå®—æ¬²å’Œé£é™©çº§åˆ«é€‰æ‹©åæ£€æµ‹è„šæœ¬å¼ºåº¦
                anti_detection_level = 'basic'
                
                # æ¬²æœ›é©±åŠ¨çš„åæ£€æµ‹ç­–ç•¥
                desire_anti_detection = {
                    'å‚²æ…¢': 'advanced',    # é€‚åº¦ä¿æŠ¤
                    'å«‰å¦’': 'maximum',     # å…¨é¢ä¿æŠ¤
                    'æ„¤æ€’': 'advanced',    # å¹³è¡¡é€Ÿåº¦å’Œä¿æŠ¤
                    'æ‡’æƒ°': 'basic',       # æœ€å°ä¿æŠ¤
                    'è´ªå©ª': 'advanced',    # å¹³è¡¡ä¿æŠ¤
                    'æš´é£Ÿ': 'maximum',     # å¿«é€Ÿä½†é«˜å¼ºåº¦
                    'è‰²æ¬²': 'maximum'      # æè‡´ä¿æŠ¤
                }
                
                # å…ˆåº”ç”¨æ¬²æœ›é©±åŠ¨çš„åæ£€æµ‹çº§åˆ«
                anti_detection_level = desire_anti_detection.get(dominant_desire, 'basic')
                
                # å†æ ¹æ®é£é™©çº§åˆ«è¿›è¡Œè°ƒæ•´
                if risk_level > 0.5:
                    anti_detection_level = 'advanced'
                if risk_level > 0.8 or pattern == 'stealth':
                    anti_detection_level = 'maximum'
                
                print(f"[ä¸ƒå®—æ¬²çˆ¬è™«] {dominant_desire}æ¨¡å¼ - åæ£€æµ‹çº§åˆ«: {anti_detection_level}")
                
                # è·å–å¹¶æ‰§è¡Œç›¸åº”çº§åˆ«çš„åæ£€æµ‹è„šæœ¬
                anti_detection_script = self.fingerprint_spoofer.get_anti_detection_script(level=anti_detection_level)
                page.evaluate_on_new_document(anti_detection_script)
                
                # æ ¹æ®é£é™©çº§åˆ«å†³å®šæ˜¯å¦æ·»åŠ CanvasæŒ‡çº¹æ··æ·†
                if risk_level > 0.3:
                    page.evaluate_on_new_document(self.fingerprint_spoofer.get_canvas_fingerprint_confusion_script())
                
                # æ ¹æ®é£é™©çº§åˆ«å†³å®šæ˜¯å¦æ·»åŠ WebGLæŒ‡çº¹æ··æ·†
                if risk_level > 0.4:
                    page.evaluate_on_new_document(self.fingerprint_spoofer.get_webgl_fingerprint_confusion_script())
                
                # è®¾ç½®è¯·æ±‚æ‹¦æˆªå™¨
                page.route("**/*", lambda route, request: self._playwright_request_handler(route, request))
                
                # è®°å½•ç½‘ç»œè¯·æ±‚ä»¥ä¾¿åˆ†æ
                all_responses = []
                blocked_requests = []
                
                def response_handler(response):
                    all_responses.append(response)
                    # è®°å½•è¢«é˜»æ­¢çš„è¯·æ±‚
                    if response.status in [403, 429, 503]:
                        blocked_requests.append({
                            'url': response.url,
                            'status': response.status,
                            'headers': dict(response.headers)
                        })
                
                page.on('response', response_handler)
                
                # ä½¿ç”¨ä¼˜åŒ–çš„è¯·æ±‚é“¾
                request_chain = self._generate_optimized_request_chain(url)
                referrer = None
                
                # è®¡ç®—è¯·æ±‚é“¾é•¿åº¦ - åŸºäºé£é™©çº§åˆ«
                chain_length = len(request_chain) - 1  # ä¸åŒ…æ‹¬ç›®æ ‡URL
                if risk_level > 0.7:
                    # é«˜é£é™©æ—¶å¢åŠ é¢„çƒ­é¡µé¢æ•°é‡
                    chain_length = min(chain_length + 2, len(request_chain) - 1)
                elif risk_level < 0.3:
                    # ä½é£é™©æ—¶å¯ä»¥å‡å°‘é¢„çƒ­é¡µé¢
                    chain_length = max(1, chain_length - 1)
                
                # æ‰§è¡Œè¯·æ±‚é“¾ä¸­çš„æ±¡æŸ“èµ„æºè¯·æ±‚
                for chain_url in request_chain[:chain_length]:
                    try:
                        # è®¾ç½®è¯·æ±‚å¤´ï¼ŒåŒ…æ‹¬åŠ¨æ€ç”Ÿæˆçš„referrer
                        page.set_extra_http_headers(
                            self.fingerprint_spoofer.generate_dynamic_headers(chain_url, referrer)
                        )
                        
                        # æ ¹æ®è¡Œä¸ºæ¨¡å¼å†³å®šç­‰å¾…ç­–ç•¥
                        wait_until = 'domcontentloaded'  # é»˜è®¤
                        if pattern == 'careful' or pattern == 'stealth':
                            wait_until = 'networkidle'  # æ›´å½»åº•ç­‰å¾…
                        elif pattern == 'hurried':
                            wait_until = 'commit'  # æ›´å¿«å¯¼èˆª
                        
                        page.goto(chain_url, wait_until=wait_until, timeout=5000)
                        
                        # åŸºäºå…ƒè®¤çŸ¥å†³ç­–å’Œç¯å¢ƒé£é™©åŠ¨æ€è°ƒæ•´äº¤äº’å¼ºåº¦
                        context_for_decision = {
                            'url': chain_url,
                            'risk_level': risk_level,
                            'behavior_pattern': pattern,
                            'content_type': page.evaluate('() => document.contentType'),
                            'element_count': page.evaluate('() => document.querySelectorAll("*").length')
                        }
                        
                        # åŸºäºä¸ƒå®—æ¬²å’Œé£é™©çº§åˆ«å†³å®šäº¤äº’è¯¦ç»†ç¨‹åº¦
                        detailed_interaction = False
                        
                        # æ¬²æœ›é©±åŠ¨çš„äº¤äº’è¯¦ç»†åº¦
                        desire_interaction_detail = {
                            'å‚²æ…¢': False,      # é«˜æ•ˆï¼Œæ— éœ€è¯¦ç»†äº¤äº’
                            'å«‰å¦’': True,       # ä»”ç»†æ¨¡ä»¿ï¼Œè¯¦ç»†äº¤äº’
                            'æ„¤æ€’': False,      # æ€¥èºï¼Œå‡å°‘äº¤äº’
                            'æ‡’æƒ°': False,      # æœ€å°åŒ–äº¤äº’
                            'è´ªå©ª': True,       # å¹³è¡¡äº¤äº’
                            'æš´é£Ÿ': False,      # å¿«é€Ÿäº¤äº’
                            'è‰²æ¬²': True        # ä¸“æ³¨ï¼Œè¯¦ç»†äº¤äº’
                        }
                        
                        # å…ˆåº”ç”¨æ¬²æœ›é©±åŠ¨çš„äº¤äº’è¯¦ç»†åº¦
                        detailed_interaction = desire_interaction_detail.get(dominant_desire, False)
                        
                        # å†æ ¹æ®é£é™©çº§åˆ«å’Œæ¨¡å¼è¿›è¡Œè°ƒæ•´
                        if risk_level > 0.6 or pattern in ['careful', 'stealth']:
                            detailed_interaction = True
                        elif risk_level > 0.3 and context_for_decision['element_count'] > 500:
                            detailed_interaction = True
                        
                        # æ ¹æ®å†…å®¹å¤æ‚åº¦ä¼°ç®—åˆé€‚çš„äº¤äº’æŒç»­æ—¶é—´
                        content_complexity = min(context_for_decision['element_count'] / 1000, 1.0)
                        base_duration = 1.0 + content_complexity * 4.0
                        
                        # åº”ç”¨å…ƒè®¤çŸ¥å†³ç­–çš„äº¤äº’
                        self.behavior_simulator.simulate_page_interaction(
                            page, 
                            context=context_for_decision,
                            detailed=detailed_interaction,
                            duration=base_duration if detailed_interaction else None
                        )
                        
                        referrer = chain_url
                    except Exception as e:
                        print(f"[PhantomCrawler] è¯·æ±‚é“¾ä¸­èµ„æºå¤±è´¥: {str(e)}")
                    
                    # åŸºäºä¸ƒå®—æ¬²å’Œé£é™©çº§åˆ«è°ƒæ•´ç­‰å¾…æ—¶é—´
                    desire_delays = {
                        'å‚²æ…¢': (0.8, 1.5),      # çŸ­å»¶è¿Ÿ
                        'å«‰å¦’': (2.0, 3.5),      # é•¿å»¶è¿Ÿ
                        'æ„¤æ€’': (0.3, 0.8),      # æçŸ­å»¶è¿Ÿ
                        'æ‡’æƒ°': (3.0, 5.0),      # æœ€é•¿å»¶è¿Ÿ
                        'è´ªå©ª': (1.0, 2.0),      # ä¸­ç­‰å»¶è¿Ÿ
                        'æš´é£Ÿ': (0.5, 1.2),      # çŸ­å»¶è¿Ÿ
                        'è‰²æ¬²': (2.5, 4.0)       # è¾ƒé•¿å»¶è¿Ÿ
                    }
                    
                    # è·å–æ¬²æœ›å¯¹åº”çš„å»¶è¿ŸèŒƒå›´
                    min_delay, max_delay = desire_delays.get(dominant_desire, (0.5, 1.5))
                    
                    # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´
                    if risk_level > 0.6:
                        min_delay = max(min_delay, 1.5)
                        max_delay = max(max_delay, 3.0)
                    
                    # åº”ç”¨å»¶è¿Ÿ
                    self.behavior_simulator.human_delay(min_delay=min_delay, max_delay=max_delay)
                
                # è®¿é—®ç›®æ ‡URL
                target_headers = self.fingerprint_spoofer.generate_dynamic_headers(url, referrer)
                page.set_extra_http_headers(target_headers)
                
                # æ ¹æ®è¡Œä¸ºæ¨¡å¼å’Œé£é™©çº§åˆ«è°ƒæ•´ç­‰å¾…ç­–ç•¥å’Œè¶…æ—¶æ—¶é—´
                wait_strategy = 'networkidle'  # é»˜è®¤æ›´ä¿å®ˆ
                timeout_value = global_config.get('playwright.timeout', 30000)
                
                if pattern == 'hurried' and risk_level < 0.4:
                    wait_strategy = 'domcontentloaded'  # å¿«é€Ÿæµè§ˆæ¨¡å¼
                    timeout_value = 20000
                elif (pattern == 'careful' or pattern == 'stealth') or risk_level > 0.6:
                    wait_strategy = 'networkidle'  # å½»åº•ç­‰å¾…
                    timeout_value = 45000  # æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                
                page.goto(url, wait_until=wait_strategy, timeout=timeout_value)
                
                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                status_code = page.status
                
                # æ£€æµ‹æ˜¯å¦å­˜åœ¨éªŒè¯ç 
                is_captcha = self.fingerprint_spoofer.is_captcha_page(content)
                is_blocked = self._is_blocked_content(content)
                
                # åŸºäºæ£€æµ‹ç»“æœæ›´æ–°å…ƒè®¤çŸ¥ç³»ç»Ÿ
                detection_info = {
                    'blocked': is_blocked or is_captcha,
                    'captcha_detected': is_captcha,
                    'status_code': status_code,
                    'url': url,
                    'blocked_requests_count': len(blocked_requests)
                }
                
                # å¦‚æœæ£€æµ‹åˆ°é˜»æ­¢ï¼Œæ‰§è¡Œå…ƒè®¤çŸ¥è‡ªé€‚åº”
                if is_blocked or is_captcha:
                    print(f"[PhantomCrawler] {is_captcha and 'æ£€æµ‹åˆ°éªŒè¯ç é¡µé¢' or 'æ£€æµ‹åˆ°è¢«é˜»æ­¢å†…å®¹'}")
                    
                    # è®°å½•åˆ°å…ƒè®¤çŸ¥ç³»ç»Ÿ
                    detection_type = 'captcha_detected' if is_captcha else 'content_blocked'
                    self.seven_desires.record_detection_attempt(url, detection_type)
                    
                    # æ›´æ–°é£é™©çº§åˆ«
                    risk_increase = 0.3 if is_captcha else 0.2
                    self.seven_desires.update_risk_level(url, risk_increase)
                    
                    # æ‰§è¡Œå…ƒè®¤çŸ¥è‡ªé€‚åº”
                    self._metacognitive_adaptation(url, detection_info)
                    
                    # æ ¹æ®é£é™©çº§åˆ«å†³å®šå†·å´æ—¶é—´
                    base_cooldown = 10 if is_captcha else 5
                    cooldown_multiplier = 1 + risk_level  # é£é™©è¶Šé«˜ï¼Œå†·å´æ—¶é—´è¶Šé•¿
                    cooldown_time = random.uniform(base_cooldown, base_cooldown + 10) * cooldown_multiplier
                    
                    print(f"[PhantomCrawler] å†·å´æ—¶é—´: {cooldown_time:.2f}ç§’")
                    time.sleep(cooldown_time)
                
                # åº”ç”¨å…ƒè®¤çŸ¥åæ£€æµ‹ç­–ç•¥
                # æ³¨å…¥åæ£€æµ‹è„šæœ¬
                if hasattr(self.fingerprint_spoofer, 'get_anti_detection_script'):
                    script_level = 'basic'
                    if risk_level > 0.7:
                        script_level = 'maximum'
                    elif risk_level > 0.4:
                        script_level = 'advanced'
                    
                    anti_detection_script = self.fingerprint_spoofer.get_anti_detection_script(level=script_level)
                    if anti_detection_script:
                        page.evaluate(anti_detection_script)
                        print(f"[PhantomCrawler] æ³¨å…¥{script_level}çº§åæ£€æµ‹è„šæœ¬")
                
                # æ ¹æ®å…ƒè®¤çŸ¥åˆ†æè°ƒæ•´äº¤äº’ç­–ç•¥
                interaction_probability = 0.8  # é»˜è®¤80%æ¦‚ç‡æ‰§è¡Œå®Œæ•´äº¤äº’
                if risk_level > 0.7:
                    interaction_probability = 1.0  # é«˜é£é™©æ—¶æ€»æ˜¯æ‰§è¡Œäº¤äº’
                elif pattern == 'hurried' and risk_level < 0.3:
                    interaction_probability = 0.5  # ä½é£é™©å¿«é€Ÿæ¨¡å¼å‡å°‘äº¤äº’
                
                if random.random() < interaction_probability:
                    # æ ¹æ®é£é™©çº§åˆ«å†³å®šäº¤äº’ç»†èŠ‚
                    if risk_level > 0.5 or pattern == 'careful' or pattern == 'stealth':
                        # é«˜é£é™©æˆ–è°¨æ…æ¨¡å¼ä¸‹æ‰§è¡Œè¯¦ç»†äº¤äº’
                        self.behavior_simulator.simulate_page_interaction(page, detailed=True)
                        
                        # åŸºäºå…ƒè®¤çŸ¥åˆ†æçš„é˜…è¯»è¡Œä¸º
                        content_metrics = page.evaluate('''() => {
                            const paragraphs = document.querySelectorAll('p').length;
                            const textLength = document.body.textContent ? document.body.textContent.length : 0;
                            const isArticle = document.querySelector('article, .article, .content, .main-content') !== null;
                            return {
                                paragraphs: paragraphs,
                                textLength: textLength,
                                isArticle: isArticle
                            };
                        }''')
                        
                        # è®¡ç®—é˜…è¯»æ—¶é—´
                        base_reading_time = 2.0
                        if content_metrics['textLength'] > 10000:
                            base_reading_time = 10.0
                        elif content_metrics['textLength'] > 5000:
                            base_reading_time = 7.0
                        elif content_metrics['textLength'] > 2000:
                            base_reading_time = 5.0
                        elif content_metrics['textLength'] > 1000:
                            base_reading_time = 3.0
                        
                        # æ ¹æ®é£é™©çº§åˆ«è°ƒæ•´
                        if risk_level > 0.7:
                            base_reading_time *= 1.5
                        elif pattern == 'hurried':
                            base_reading_time *= 0.7
                        
                        self.behavior_simulator.simulate_reading_behavior(page, duration=base_reading_time)
                    else:
                        # åŸºæœ¬äº¤äº’
                        self.behavior_simulator.simulate_page_interaction(page)
                        
                        # åŸºæœ¬æ¨¡å¼ä¸‹ä¹Ÿæ ¹æ®å†…å®¹è°ƒæ•´é˜…è¯»æ—¶é—´
                        text_length = page.evaluate('() => document.body.textContent ? document.body.textContent.length : 0')
                        base_duration = max(1.0, min(3.0, text_length / 1000))
                        self.behavior_simulator.simulate_reading_behavior(page, duration=base_duration)
                else:
                    # æœ€å°åŒ–äº¤äº’æ¨¡å¼
                    self.behavior_simulator._simulate_mouse_movement(page)
                    self.behavior_simulator._simulate_scrolling(page)
                
                # éšæœºé¢å¤–ç­‰å¾…ï¼ŒåŸºäºé£é™©çº§åˆ«
                if random.random() < 0.3 + (risk_level * 0.4):  # é£é™©è¶Šé«˜ï¼Œç­‰å¾…æ¦‚ç‡è¶Šå¤§
                    extra_wait = random.uniform(1, 3)
                    if risk_level > 0.6:
                        extra_wait *= 2  # é«˜é£é™©æ—¶ç­‰å¾…æ›´ä¹…
                    time.sleep(extra_wait)
                
                # è·å–æœ€ç»ˆå†…å®¹
                final_content = page.content()
                final_status = page.status
                
                # è·å–cookieså’Œå“åº”ä¿¡æ¯
                cookies = page.context.cookies()
                cookies_dict = {cookie['name']: cookie['value'] for cookie in cookies}
                
                # å¤„ç†å“åº”å¤´å’ŒçŠ¶æ€ç 
                headers = {}
                response_status = None
                
                # æŸ¥æ‰¾ç›®æ ‡URLçš„å“åº”
                for response in all_responses:
                    if response.url == url or url in response.url:
                        headers = dict(response.headers)
                        response_status = response.status
                        break
                
                if response_status is None:
                    response_status = final_status
                
                # è°ƒç”¨å›è°ƒ
                if callback:
                    callback(page)
                
                # éšæœºå…³é—­æ–¹å¼ï¼ŒåŸºäºé£é™©çº§åˆ«
                if random.random() < 0.7:
                    page.close()
                    context.close()
                browser.close()
                
                # è®¡ç®—å“åº”æ—¶é—´
                response_time = time.time() - start_time
                
                # å‡†å¤‡ç»“æœ
                result = {
                    'url': url,
                    'status_code': response_status or final_status,
                    'content': final_content,
                    'headers': headers,
                    'cookies': cookies_dict,
                    'playwright_used': True,
                    'user_agent': fingerprint_headers['User-Agent'],
                    'fingerprint_details': {
                        'viewport': {'width': width, 'height': height},
                        'user_agent': fingerprint_headers['User-Agent']
                    },
                    'response_time': response_time,
                    'blocked': is_blocked or is_captcha,
                    'captcha_detected': is_captcha,
                    'risk_level': risk_level
                }
                
                # è®°å½•å†å²
                self.crawl_history.append({
                    'url': url,
                    'status_code': response_status or final_status,
                    'timestamp': time.time(),
                    'session_id': self.session_id,
                    'blocked': is_blocked or is_captcha,
                    'response_time': response_time,
                    'playwright_used': True,
                    'risk_level': risk_level
                })
                
                # æ‰§è¡Œå…ƒè®¤çŸ¥åˆ†æ
                self._metacognitive_analysis(url, result, response_time)
                
                # æ›´æ–°ç¯å¢ƒæ„ŸçŸ¥
                self.behavior_simulator._update_environment_awareness(result)
                
                # å¦‚æœæœªè¢«é˜»æ­¢ä¸”è¿ç»­æˆåŠŸï¼Œè€ƒè™‘é™ä½é£é™©è¯„ä¼°
                if not result['blocked'] and hasattr(self, 'success_streak') and self.success_streak > 3:
                    self.seven_desires.update_risk_level(url, -0.1)  # ç•¥å¾®é™ä½é£é™©è¯„ä¼°
                
                return result
                
        except Exception as e:
            error_msg = str(e)
            print(f"[PhantomCrawler] Playwrightå¤‡ç”¨çˆ¬å–å¤±è´¥: {error_msg}")
            
            # åˆ†æé”™è¯¯ç±»å‹
            error_type = 'unknown_error'
            error_str = str(error_msg).lower() if error_msg else ''
            if 'timeout' in error_str:
                error_type = 'timeout_error'
            elif 'connection' in error_str:
                error_type = 'connection_error'
            elif 'network' in error_str:
                error_type = 'network_error'
            elif 'browser' in error_str:
                error_type = 'browser_error'
            elif 'captcha' in error_str or 'block' in error_str:
                error_type = 'detection_error'
            
            # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­–ç•¥
            self._adjust_strategy_based_on_error(error_type)
            
            # è®°å½•å¤±è´¥åˆ°å…ƒè®¤çŸ¥ç³»ç»Ÿ
            self._record_failure(url, error_msg)
            
            # æ›´æ–°é£é™©çº§åˆ« - åŸºäºé”™è¯¯ç±»å‹
            if error_type == 'detection_error':
                self.seven_desires.update_risk_level(url, 0.3)  # æ˜¾è‘—å¢åŠ é£é™©
            elif error_type in ['browser_error', 'network_error']:
                self.seven_desires.update_risk_level(url, 0.1)  # è½»å¾®å¢åŠ é£é™©
            
            # å°è¯•åˆ‡æ¢è¡Œä¸ºæ¨¡å¼
            if error_type == 'detection_error' and hasattr(self, 'behavior_simulator'):
                self.behavior_simulator.shift_behavior_pattern()
            
            # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚æ•è·å¤„ç†ï¼Œä½†æ ‡è®°å·²ç»å°è¯•è¿‡playwright
            raise Exception(f"Playwrightçˆ¬å–å¤±è´¥: {error_msg}") from e
    
    def crawl_batch(self, urls: List[str], max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """æ‰¹é‡çˆ¬å–å¤šä¸ªURL"""
        results = []
        
        # å°†URLåˆ†æˆæ‰¹æ¬¡
        for i in range(0, len(urls), max_concurrent):
            batch = urls[i:i+max_concurrent]
            batch_results = []
            
            for url in batch:
                result = self.crawl(url)
                batch_results.append(result)
                
                # æ‰¹æ¬¡å†…çš„URLä¹‹é—´æ·»åŠ å»¶æ—¶
                if url != batch[-1]:
                    self.behavior_simulator.human_delay()
            
            results.extend(batch_results)
            
            # æ‰¹æ¬¡ä¹‹é—´æ·»åŠ æ›´é•¿çš„å»¶æ—¶
            if i + max_concurrent < len(urls):
                time.sleep(random.uniform(5, 10))
        
        return results
    
    def _playwright_request_handler(self, route, request):
        """Playwrightè¯·æ±‚å¤„ç†å‡½æ•°ï¼Œç”¨äºæ‹¦æˆªå’Œä¿®æ”¹è¯·æ±‚"""
        # è·³è¿‡æŸäº›èµ„æºç±»å‹ä»¥æé«˜æ€§èƒ½
        skip_resource_types = global_config.get('playwright.skip_resource_types', ['image', 'media', 'font'])
        
        # æ™ºèƒ½è·³è¿‡èµ„æº - åŸºäºURLæ¨¡å¼
        if request.resource_type in skip_resource_types:
            # ä½†å…è®¸ä¸€äº›é‡è¦çš„å›¾ç‰‡èµ„æºé€šè¿‡ï¼ˆå¦‚éªŒè¯ç ï¼‰
            url = request.url
            important_image_patterns = ['captcha', 'verify', 'security', 'auth']
            if not any(pattern in url.lower() for pattern in important_image_patterns):
                route.abort()
                return
        
        # ç”ŸæˆåŠ¨æ€è¯·æ±‚å¤´
        headers = dict(request.headers)
        
        # ç§»é™¤å¯èƒ½æš´éœ²è‡ªåŠ¨åŒ–çš„å¤´éƒ¨
        headers_to_remove = ['X-Powered-By', 'Server', 'X-AspNet-Version']
        for header in headers_to_remove:
            if header in headers:
                del headers[header]
        
        # æ™ºèƒ½ä¿®æ”¹å¤´éƒ¨ - åŸºäºè¯·æ±‚ç±»å‹
        if request.resource_type == 'document':
            # ä¸ºä¸»è¦æ–‡æ¡£è¯·æ±‚æ·»åŠ æ›´å®Œæ•´çš„å¤´éƒ¨
            headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            headers['Accept-Language'] = random.choice(['en-US,en;q=0.9', 'zh-CN,zh;q=0.9', 'ja-JP,ja;q=0.9'])
            headers['Cache-Control'] = random.choice(['max-age=0', 'no-cache'])
            
            # åªå¯¹éHTTPSè¯·æ±‚æ·»åŠ è¿™ä¸ªå¤´éƒ¨
            if not request.url.startswith('https'):
                headers['Upgrade-Insecure-Requests'] = '1'
        
        elif request.resource_type == 'script':
            # ä¸ºè„šæœ¬è¯·æ±‚æ·»åŠ åˆé€‚çš„å¤´éƒ¨
            headers['Accept'] = '*/*'
            headers['Accept-Encoding'] = 'gzip, deflate, br'
        
        elif request.resource_type == 'xhr' or request.resource_type == 'fetch':
            # ä¸ºAPIè¯·æ±‚æ·»åŠ é€‚å½“çš„å¤´éƒ¨
            headers['X-Requested-With'] = 'XMLHttpRequest'
            headers['Accept'] = 'application/json, text/javascript, */*; q=0.01'
        
        # éšæœºæ·»åŠ æˆ–ä¿®æ”¹ä¸€äº›å…¶ä»–å¤´éƒ¨
        if random.random() < 0.3:
            headers['DNT'] = '1'
        
        # æ·»åŠ è¯·æ±‚ç­¾åå‚æ•°
        if random.random() < 0.4 and request.resource_type == 'xhr':
            from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
            
            parsed_url = urlparse(request.url)
            query_params = parse_qs(parsed_url.query)
            
            # ç”Ÿæˆè¯·æ±‚ç­¾å
            signature = self.fingerprint_spoofer.generate_request_signature()
            
            # éšæœºé€‰æ‹©å‚æ•°å
            param_names = ['_', 't', 'v', 'uid', 'r', 's']
            param_name = random.choice(param_names)
            query_params[param_name] = [signature['nonce']]
            
            # é‡æ–°æ„å»ºURL
            new_query = urlencode(query_params, doseq=True)
            new_url = urlunparse(
                (parsed_url.scheme, parsed_url.netloc, parsed_url.path, 
                parsed_url.params, new_query, parsed_url.fragment)
            )
            
            route.continue_(url=new_url, headers=headers)
        else:
            route.continue_(headers=headers)
            
    def close(self) -> None:
        """å…³é—­çˆ¬è™«ï¼Œæ¸…ç†èµ„æº"""
        if self.http_client:
            self.http_client.close()
        
        if self.playwright_browser:
            # å…³é—­Playwrightæµè§ˆå™¨
            pass
        
        self.is_running = False
        print(f"[PhantomCrawler] å·²å…³é—­ï¼Œä¼šè¯ID: {self.session_id}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'session_id': self.session_id,
            'crawl_count': len(self.crawl_history),
            'is_running': self.is_running,
            'behavior_stats': self.behavior_simulator.get_behavior_statistics(),
            'proxy_count': len(self.protocol_obfuscator.proxy_chain)
        }